[{"@type": "Post", "title": "TerminusDB Internals part 3: Sorting Every Sort of Thing", "content": "\nSome of the the original experiments with TerminusDB were in postgres,\nwhere we built a table of IRIs and ids, and then created a\nmulti-indexed table of triples. We then compared the speed of this to\na library called HDT which created a compact representation of graphs,\nand found HDT to be extremely fast for large RDF databases (Think TTL\nfiles on the order of 100GB).\n\nThis got us thinking seriously about succinct data structures, and so\nTerminusDB has used the ideas in HDT as a starting point.\n\nOne of the choices made by HDT is the [front coded\ndictionary](https://en.wikipedia.org/wiki/Incremental_encoding). This\ntends to work very well for IRIs, since we tend to share addresses as\nprefixes, leading to substantial compression and reasonable retrieval\nspeed.\n\nThe dictionary reliese on lexical sorting of data, as we want to\nbranch at points which share prefixes. So sorting lexically helps us\nto maximally share prefixes.\n\nLexical ordering also allows us to start with a prefix, and iterate\nover everything which shares it, or to perform fast range queries,\nsimply by finding the beginning point, and iterating until we are\nat the terminus of the range.\n\nSo we get:\n\n* Compression\n* Log like access\n* Range queries\n\nBut not everything is naturally designed to be stored lexically. Take\nthe classic example of the directory in linux:\n\n\n```shell\ngavin@titan:~/tmp/sort$ touch 2\ngavin@titan:~/tmp/sort$ touch 11\ngavin@titan:~/tmp/sort$ touch 10\ngavin@titan:~/tmp/sort$ touch 100\ngavin@titan:~/tmp/sort$ touch 101\ngavin@titan:~/tmp/sort$ touch 110\ngavin@titan:~/tmp/sort$ ls -la\ntotal 8\ndrwxrwxr-x  2 gavin gavin 4096 Okt 31 11:59 .\ndrwxrwxr-x 14 gavin gavin 4096 Okt 31 11:59 ..\n-rw-rw-r--  1 gavin gavin    0 Okt 31 11:59 1\n-rw-rw-r--  1 gavin gavin    0 Okt 31 11:59 10\n-rw-rw-r--  1 gavin gavin    0 Okt 31 11:59 100\n-rw-rw-r--  1 gavin gavin    0 Okt 31 11:59 101\n-rw-rw-r--  1 gavin gavin    0 Okt 31 11:59 11\n-rw-rw-r--  1 gavin gavin    0 Okt 31 11:59 110\n-rw-rw-r--  1 gavin gavin    0 Okt 31 11:59 2\n```\n\nIn this world 11 is less than 101, and 2 is greater than 110. Not what\nwe typically want when we are sorting numbers.\n\n## Lexical Everything\n\nBut as it turns out, numbers can also be sorted lexically, provided we\nstore them in a clever way. These lexical ordering tricks can give us\nid<->data conversion using dictionaries, which allows for compression,\nprefix queries and range queries.\n\n### Integers\n\nSo how do we get 100 to be larger than 2? If we have *fixed* size\nintegers, such as Int32, the answer is relatively simple. We break\nInt32 into 4 bytes written out in [big\nendian](https://en.wikipedia.org/wiki/Endianness). We're almost done\nsave one complication. In most representations, we keep around a\n*sign* bit on integers, generally stored in the most significant\nposition, which is 1 if the number is negative, and 0 if it is\npositive.\n\nThis is terrible, since all negative numbers are now larger than\npositive numbers. Further, integers are generally stored in a [two's\ncomplement](https://en.wikipedia.org/wiki/Two%27s_complement), meaning\nthat we flip every bit of a negative number. This is actually a *good*\nthing. Because it means that smaller numbers are bigger, and bigger\nnumbers are smaller. Which is exactly how we expect negative numbers\nto sort! That is, -10 should be smaller than -1.\n\n```rust\n-1 = 0bffff_fffe\n```\n\nTo fix the sign problem is simple. We just flip the sign bit and we\nare done! We now have lexically sortable integers.\n\n### Floats\n\nIEEE Floating point numbers are also surprisingly simple to sort\nlexically. We have the same trick, requiring a sign flip, but in the\ncase of negative numbers, we actually have to put them in the twos\ncomplement representation, as floating point can't avail of the same\ntwos complement tricks used in integer arithmetic, so this is computed\nexternally.\n\nThis is all there is to it in rust (using the `bytes_order` library to\nensure we get a big endian representation).\n\n```rust\nconst F32_SIGN_MASK: u32 = 0x8000_0000;\nconst F32_COMPLEMENT: u32 = 0xffff_ffff;\nfn float32_to_vec(f: &f32) -> Vec<u8> {\n    let g: f32 = if f.to_bits() & F32_SIGN_MASK > 0 {\n        f32::from_bits(f.to_bits() ^ F32_COMPLEMENT)\n    } else {\n        f32::from_bits(f.to_bits() ^ F32_SIGN_MASK)\n    };\n    let mut wtr = Vec::with_capacity(4);\n    wtr.write_f32::<BigEndian>(g).unwrap();\n    wtr\n}\n```\n\nNotice, flipping all of the bits is just an\n[xor](https://en.wikipedia.org/wiki/Exclusive_or) with a complement\nmask which has every bit set.\n\nPerhaps surprisingly, this trick also works for NaN and Negative and\nPositive Infinity!\n\n### BigInts\n\nBut what about big integers? If the size is not fixed, what are we to\ndo? Well, we could find the largest number and store everything in the\nnumber of bits required by this largest number, and use the\nrepresentation above. However, this threatens to use up a lot of space.\n\nInstead, we can simply *prefix* integers with their size. Conceptually\nwe can rewrite our directory files from above as:\n\n```shell\ngavin@titan:~/$ python\n>>> x = [\"0_1\", \"2_10\",\"3_100\",\"3_101\",\"2_11\",\"0_2\"]\n>>> x.sort()\n>>> x\n['0_1', '0_2', '2_10', '2_11', '3_100', '3_101']\n```\n\nPresto! It works!\n\nBut what about negative numbers? Well, we can perform the same sorts\nof trick with complements. Let's do a complement in base 10 to see how\nit works.\n\nFirst, let's add a few more numbers to our list. How about -1, -2, and\n-10. Now, we can represent a negative with an `-` character which is\nless than every number in ascii. Then we can take our size, and\ncomplement it with 9, so that 9 is 0, and 0 is 9, and every other\ndigit is in between.\n\nOk, so -1 becomes `-9_9`. -2 is `-9_8`. And -10 is `-8_89`.\n\n\n```shell\ngavin@titan:~/$ python\n>>> x = [\"0_1\", \"2_10\",\"3_100\",\"3_101\",\"2_11\",\"0_2\", \"-9_9\", \"-9_8\", \"-8_89\"]\n>>> x.sort()\n>>> x\n['-8_89', '-9_8', '-9_9', '0_1', '0_2', '2_10', '2_11', '3_100', '3_101']\n```\n\nGreat! We have an encoding that puts -10 on the bottom, and -1 on the\ntop of the negatives.\n\nOf course, if you're paying close attention, you'll see that we need a\nway to represent sizes that can go bigger than 9. We need a lexically\nsortable kind of size signifier. There are lots of ways to do this,\nwith the simplest being unary. You simply take the size and represent\nit with a number of 1s and a self-delimiting zero. So the size 3\nbecomes `1110`.  This is pretty big, and a bit awkward, plus it\ndoesn't respect byte alignment, but is manageable in certain settings.\n\nIn TerminusDB we instead use an encoding which has a *continuation\nbit*. That is, we represent numbers up to 128 with the bits of the\nnumber in binary. So 3 would be `0b000_0011`. We then stick a bit at\nthe top which is *zero* if the number is less than 128 and *one* if\nwe're only representing the top 7 bits of our number, and our number\nis greater than 128 in which case we need another byte. Now, our\nencoded 3 becomes the entire byte: `[0b0000_0011]`. One can easily see\nthat anything greater than 128 will already compare as larger than\nanything less than 128 because it will have its most significant bit\nset. 129 becomes the two bytes: `[0b1000_0001, 0b0000_0001]`.\n\nThis kind of trick is called\n[variable-byte](https://en.wikipedia.org/wiki/Variable-length_quantity)\nencoding, and sorts the orders first, and the numbers inside of that\norder.\n\n### Decimal\n\nTerminusDB implements the XSD data types. And one of these data types\nis the rather unusual `xsd:decimal` which codes arbitrary precision\ndecimal numbers. It's unsual because most database do not store\naribtrary precision floating points, and if they do, they generally do\nso in binary. While often times people *want* information recorded in\nthe base-10 they are familiar with, a lot of our computing\ninfrastructure makes this somewhat awkward.\n\nAs it turns out, you can *also* store these lexically, provided we do\na bit of monkey-business.\n\nIn TerminusDB we store these with a pair of elements concatenated. The\nfirst is a bignum, which stores everything before the full stop. The\nsecond part is a v-byte style binary-coded decimal, which packs two\ndecimal characters per byte. This represents *no* decimal as the\nlowest element, and a single digit decimal interleaved between\ntwo-digit decimals.\n\n| decimal | encoding |\n|---------|----------|\n|   none  |    0     |\n|   0     |    1     |\n|   00    |    2     |\n|   01    |    3     |\n|   02    |    4     |\n|   ...   |   ...    |\n|   1     |    12    |\n|   10    |    13    |\n|   ...   |   ...    |\n|   9     |   100    |\n|   90    |   101    |\n|   ...   |   ...    |\n|   99    |   111    |\n\nWe leave the *last* bit as a continuation bit for our v-byte encoding,\nas we need to compare as larger, only if the rest of the byte is the\nsame.\n\nSchematically:\n\n```diagram\n\n   BCD\n    |     continuation bit\n    |     |\n| xxxxxxx c | xxxxxxx c | ...\n```\n\nThis representation also allows us to keep significant digits, for\ninstance `0.0` will be encoded differently from `0` and from `0.00`,\nwhile retaining appropriate lexical sorting. This is important in\nscientific applications where significance should be recorded.\n\n### Dates\n\n`xsd:dateTime` is a format which is based on ISO 8660, and is actually\nalready *mostly* lexical. However it is broken in that the first\nelement can be negative, can be larger than 4 digits, and can have a\ntime-zone offset, and an arbitrary precision float for sub-second\nportions of the date.\n\nFurther, the format is a string, which is far too large to efficienty represent\nthe data contained.\n\nIn TerminusDB we deal with this simply by converting the number into a\ndecimal encoding of the number of seconds since 1970 with an arbitrary\nprecision. This can encode the femto-seconds of a laser pulse to the\npoint at which the universe started in a format that is range\nquerable, without breaking the bank in terms of size!\n\nDoes any other database do this?\n\n### Other types\n\nWe've also played around with some other types, including tuples and\neven dictionaries, all of which support a lexical ordering and range\nqueries. We haven't implemented them as we're not sure about the\nuse-cases, but it's certainly interesting.\n\n### Even Smaller\n\nOf course it's possible to get smaller and faster on all of these\ndatatypes with appropriate datastructures, and we're keen on plumbing\nthe depths of tiny as TerminusDB matures. If you've a favourite\nencoding approach which is also fast and supports range queries, or\ncan think of other interesting data types which can be supported, give\nus a shout!\n", "date": "2023-01-13T19:46:13.033160", "feature": null}, {"@type": "Post", "title": "Excel as code", "content": "\nExcel is one of the most widely used software products in the entire\nworld.  Wordprocessors have *more* users to be sure, but, Excel is\n*nothing* like a word processor. It is in reality a programming\nlanguage and database combined.\n\nNot counting Excel users, there are only about 30 million\nprogrammers. Estimates put the number of Excel users between 500m and\nover 1 billion!\n\nIt is therefore, by far, the most used programming language on the\nplanet. It is easily 20 times more popular than the next contender.\n\nExcels are running the core of a huge number of business functions\nfrom budgeting, product management, customer accounts, and many many\nother things besides.\n\nThe value of Excel is that it is presenting the data, with a set of\nformulae that let you keep derived data up-to-date. This *inferred*\ndata provides sums and computations, sometimes simple, but sometimes\nexquisitely complex.\n\nAnd through this whole range of complexity, with half a billion users,\nvirtually nobody treats Excel *seriously* like a programming language.\n\nHow can this be? We have a programming language which is *essentially*\nacting as a declarative database, and yet we don't do unit tests, we\ndon't keep track of changes, we collaborate it by sending it in them\nmail and god-forbid we should doing any serious *linting* of what is\nin the thing.\n\nThis is a really crazy situation.\n\nThe programmers and database managers will often look at this\nsituation in terror and tell excel-jockeys they need to get off excel\nASAP.\n\nThe excel-jockeys might look at the database nerds and IT geeks and\nthink that they must be off their rocker. Or maybe they even feel\nashamed but realise that there is no way they are going to be able to\ndo the their job properly by simply switching to using Oracle &\nPython.\n\nOf course anyone who has used Excel in anger realises *why* it is so\nbrilliant. Show me another declarative constraint based, data driven\ninferrence language that I can teach to my grandmother and I'll eat my\nhat!\n\nPeople refuse to stop using Excel because it empowers them and they\nsimply don't want to be disempowered.\n\nAnd right they are. The problem isn't Excel. The problem is that we\nare treating Excel like its a word processor, and not what it is: a\nprogramming language.\n\n## The Programming Enlightement\n\nIn the dark ages of programming you had a source tree and you edited\nfiles in some terrible text editor and then ran a compiler. Some time\nlater you'd have a binary that you'd run and see if it crashed. If\neverything went well you might share the file on a file server with\nyour colleagues. They also changed it so you had to figure out how not\nto break everything and paste their changes back into your source tree\n(or vice versa).\n\nThis was clearly a disaster, leading to huge pain in getting the\nsource code merges to line up without failure.\n\nEnter revision control.\n\nPeople realised that there needed to be a system of checking files in\nand out such that changes could be compared and collisions could be\navoided.\n\nAnd never did the person have to leave programming in their favourite\neditor. Nobody told them to store their code in Oracle. Nobody said\nthey should share their source code in Google Docs.\n\nThis enabled vast improvements in collaboration. Fearless editing of\nfiles created a much more open development environment. You could go\nahead and make that change you knew had to cut across half of the code\nbecause you could figure out how to merge it when the time came. The\nnumber of programmers you could have working on a code base with much\nlower communication overhead increased tremendously.\n\nThe revision control system enabled a completely new approach to\nsoftware development: Continuous Integration / Continuous Deployment\n(CI/CD). CI/CD meant that when code was checked in, a series of hooks\nthat ran unit tests could be run. Linters could be run over the\nchecked in version. You could even have complex integration tests\nrunning which checked if the software still worked properly with other\nprocesses.\n\nAll of these checks meant that the *health* of the code could be known\nup to the minute. It was still possible to introduce breaking changes\nby messing something up in a clever way, but a *huge class of errors*\nwas removed.\n\n## How Excel can join the Rennaisance\n\nUnfortunately, none of this applies to Excel because Excel doesn't\nwork well with revision control.\n\nWhy?\n\nBecause Excel is not a source file. It is a database coupled with\ncode. Git was not built for this - it knows about lines in a file and\nthats it. Good luck trying to use git to resolve merge conflicts - it\nwill simply butcher your file.\n\nThe path to enlightement is a more sophisticated revision control\nsystems - ones that can understand Excel.\n\nLuckily such a thing does actually exist,\n[VersionXL](https://versionxl.com).\n\n## Collaboration\n\nThe first benefit to this new approach to putting Excel in version\ncontrol will be enabling collaboration. Sure you can send Excel files\nto people, but this is the equivalent of me e-mailing my colleague my\nsource tree every time I want to make a change.\n\nAnd if I share it with two people at once, I'm sure to end up with two\ndifferent changes. And now I must figure out how to incorporate\nboth. I've turned myself into a fault-prone (and probably very\nexpensive) revision control system. And if I make a mistake I'll be\ndigging through my e-mail looking for the one I sent to the first\nperson in order to merge the correct changes back in again.\n\nOut of the traps we are winning whenever there is a collaboration -\neven between two people. We get to merge with less hassle, and any\nmistake is just a rollback.\n\nAnd at no point did we have to leave Excel.\n\n## CI/CD for Excel\n\nNow that we have a revision control system for Excel, we can start to\nthink seriously about CI/CD and what it would mean to really treat\nExcel as code in a modern development environment.\n\nFirst off is *linting*. Linting just means writing queries or scripts\nwhich can look for obvious syntactic bugs. The value of this can not\nbe overstated. The number of stupid and obvious syntactic bugs (such\nas mispellings) that even incredibly intelligent programers make is\nhuge. And the value of noticing that even larger.\n\nWhat would Excel linting look like? It could be as simple as saying:\n\n> All currency values in this file should be in dollars\n\nOr maybe it says:\n\n> Cells in column C must be numeric.\n\nBut it could be that specific files would require custom and complex\nlinting. That's fine, that happens with code too! You should be able\nto simply at it as a test hook on commit. Once you get the green\nlight, you know that it's safe to merge.\n\nIn large corporations or organisations its often the case that you'll\neven want aspects of the layout, the number of sheets etc. to remain\nuniform even after updates. Linting can enable this to happen.\n\nOf course linting doesn't catch more complex semantic errors. For that\nwe often want to write down what we *expect* some formula to do. And\nto test that we should have a test case for our formula. This is unit\ntesting.\n\nUnit testing excel might mean ensuring certain formulae meet a set of\nexternal assertions that ensure that they still \"do the right thing\".\n\nThe value of having these external verifications might not seem\nobvious when you're calculating a total, but if the calculation is\nvery complex you probably want to have a few test cases (which might\nnot necessarily be in your workbook) to sanity test.\n\nAnd the more important the *value* of the calculations, the more\nsanity should prevail.\n\n## Conclusion\n\nExcel *is* a programming language. It's time we start treating it like\none. Excel users want to keep using the power of their favourite\nlanguage.\n\nThey don't need to change that.\n\nWhat needs to change is the idea that they are not programmers, so\nthey can join us in using modern software practices.\n", "date": "2023-01-13T19:46:13.033160", "feature": null}, {"@type": "Post", "title": "TerminusDB internals part 1: Smaller, Faster, Stronger", "content": "\nWhen we were designing TerminusDB we wanted to make a graph database\nwhich would allow curation of large complex graphs where the search\n*mode* (we'll talk a bit about modes in a minute) was not yet\nknown. In playing with some large graphs (for instance a database of\nall Polish companies since the mid 1990s) we came to the following\nconclusion:\n\n* Graphs, due to their highly interconnected nature, are hard to\n  segment\n* This makes paging, and network communication, potentially very\n  costly when edges are traversed across segments\n* Main memory size can accomodate *many* very large graphs even for\n  enterprise scale problems.\n\nPoint two is especially hard to overcome. If you start paging in a\ngraph, because of the very random nature of graphs, we can get\nextremely varied pages. This could easily lead to thrashing. Since\ndisk access times on fast SSD are one to two orders of mangitude\nslower than DRAM we're talking perhaps 100 times worse\nperformance. For networks this climbs to three orders of mangitude.\n\nThese considerations let us to speculate that we could build an\nin-memory graph database if we were careful to ensure that we were\nsparing with memory use.\n\nIn addition to this in-memory advantage, having low memory overhead\nhas additional benefits for *collaboration*, in that we can send\ndeltas around in a compact format.\n\nSize matters, and smaller is better.\n\nIt's important to note, that by *in-memory* we mean that there is no\ndatabase paging, we do not move parts of the database into and out of\nmemory. It does not mean that we are not writing to disk. TerminusDB\nis both ACID and persistent, so there is no risk of data loss for\ncompleted transactions.\n\n## Relational indexing\n\nIn typical databases, we ask questions of relations. Relations are\ntypically built from tables of facts togther with some operations\nwhich allow us to join relations. In addition we often specify\nparticular projections of these relations by asking for certain\nconstraints to hold.\n\n```SQL\nSELECT Orders.OrderID, Customers.CustomerName, Orders.OrderDate\nFROM Orders\nINNER JOIN Customers ON Orders.CustomerID=Customers.CustomerID\nWHERE Orders.OrderDate > 2010-04-01;\n```\n\n> Create a new relation by joining two relations, Customers and Orders\n> on the customer ID field and project to a relation in which all\n> dates are more recent than 2010-04-01 using the WHERE restriction.\n\nIn order to make these queries effecient we will often introduce\n*indexing* on the fields which we want to join or restrict. If we do\nnot do this, then we might have to *scan* through each `CustomerID` in\nthe one table to see if it is in the second table. A scan means we\nlook at each record. If the size of `Cusomers` is `n`\n(`|Customers| =n`) and `Orders` is `m` (`|Orders| = m`), then this\nwould result in a number of operations on the order of\n`n*m`. Similarly, if we have not indexed the date field, we will have\nto compare the date for each record that we produce.\n\nWith an index, we can make access *log like*, by creating a tree which\nmakes it possible to access the data without a scan. Our date\nrestriction will allow us to *start* our search in the right place in\nthe relation which will make our effective `m` smaller (which we might\ncall` m'`, and the order of the problem shrinks to `n * log(m')`.\n\nIf `n` and `m` are big numbers, then this is a big deal. If we have\nmore than one join, we can easily see how unindexed relations could\nlead to numbers which spiral out of control and are effectively\nimpossible to query.\n\n## Graph Databases\n\nIn a *graph database*, we remove a good deal of this complexity by\nhaving exactly one type of relation, the edge. And since we want to be\nable to explore the graph very freely, we will need to index\n*everything*. This will let us join to create paths through the graph.\nEverything is a *self* join of a single relation in a graph, the edge\nrelation.\n\nFor a search we can think of querying our relation in any of the\npossible *modes*.  A *mode* means specifying what we know, versus what\nwe need to look for. We write a `-` for anything we don't know, and a\n`+` for anything we do. We can write our modes as a triple such as\n`(+,+,-)`\n\nGraph modes include:\n\n```\n(-,-,-)\n(-,-,+)\n(-,+,-)\n(-,+,+)\n(+,-,-)\n(+,-,+)\n(+,+,-)\n(+,+,+)\n```\n...so eight query modes that we might want to support.\n\nNow, some of these might be more common than others, and therefore we\nmight want to make them faster. We'll talk about that later when we\nget into the guts of our index.\n\nBy way of example, let's think of a data point named `Joe`. If we\nwanted to see everything which `Joe` is connected to, we might say (in\nWOQL):\n\n```javascript\nlet v = Vars(\"p\",\"x\");\ntriple(\"Joe\", v.p, v.x)\n```\n\nThis has mode `(+,-,-)`.\n\nIf we wanted to see everything that was connected to everything that\nJoe was connected to, we might say:\n\n\n```javascript\nlet v = Vars(\"p\",\"x\",\"q\",\"y\");\nand(triple(\"Joe\", v.p, v.x),\n    triple(v.x, v.q, v.y))\n```\n\nHere we start with `(+,-,-)` and it might at first appear that we have\n`(-,-,-)` for our second search. However, we can get the results back\nfrom the first search to reduce this to `(+,-,-)` meaning we can\nconsider a projection of the relation speeding things up considerably.\n\n## Sparing Use of Memory\n\nDoing all of this indexing makes query very flexible. We can weave\nrelationships out of the graph and instead of having all sorts od\ndifferent tables with different shapes we can just have one big table.\n\nThis is also potentially very fast if everything is indexed and we can\ndo restrictions quickly.\n\nBut it also threatens to make things very big. How many indexes will\nwe need to build all of this?  The mode `(-,-,-)` is just the whole\nrelation so doesn't need an index. For `(+,+,+)` we can probably\npermit ourselves a scan on one of the fields as we've already\nrestricted heavily. What can we get away with?\n\nAnd indexes are *trees* which help us get log like behaviour out of\nour searches. And trees often mean adding lots of pointers. Pointers\ncan actually end up being a substantial amount of the size of an\nindexed database as we create the many layers of the tree.\n\nAnd if our database grows too great in size, we threaten to hit the\nsize of main memory. We can often add more memory, but the more\nparsimonious our representation, the more databases will fit into a\ngiven memory size. It therefore pays handsomly to be small.\n\nHoewver, we can solve this problem with a family of datastructures\nknown as *succinct data structures*. These try to keep fast, log-like\naccess modes available, while keeping storage space close to the size\nof the data itself.\n\n### The Dictionary\n\nThe first point of entry is the dictionary. Given a *name* (such as\n`Joe`) we want to come up with an internal *identifier* which consists\nof an unsigned 64 bit integer. This will be our internal name for\n`Joe`, which takes a fixed and relatively small amount of space.\n\nEssentially we need a table which goes from integers to names and back\nagain.\n\nIn TerminusDB we do this with a datastructure known as a [Front coded\ndictionary](https://en.wikipedia.org/wiki/Incremental_encoding). This\nstructure is particularly good when your identifiers are likely to\nshare prefixes. Since all named identifiers in TerminusDB are\n[IRIs](https://en.wikipedia.org/wiki/Internationalized_Resource_Identifier).\n\nIf we have a block size of eight entries, a block might look as follows:\n\n| Input     |\tCommon prefix   | Compressed output |\n|-----------|-------------------|-------------------|\n| myxa      | None              | 0 myxa            |\n| myxophyta | 'myx'             | 3 opyta           |\n| myxopod   | 'myxop'           | 5 od              |\n| nab       | None              | 0 nab             |\n| nabbed    | 'nab'             | 3 bed             |\n| nabbing   | 'nabb'            | 4 ing             |\n| nabit     | 'nabit'           | 3 it              |\n\nThe basic idea is to store deltas between strings, allowing us to use\nless space to store all of our strings than would be required with a\nstraight dictionary. The dictionary is organized in *blocks* with each\nblock starting with a full, uncompressed entry, and all subsequent\nentries in the block are allowed to refer back to any element in the\nblock to reuse parts of previous elements.\n\nAs you can see, we get substantial compression here by reusing prior\nentries. But we have to start over with no common prefix every block\nsize, and if we share nothing, we get a slight disimprovement (as we\nhave to store a zero offset as a prefix to our compressed output).\n\nThis gives reasonably good compression for typical IRIs, often between\n40 and 80%.\n\nTo get our ID, we simply need to know numerically which entry we are\nin the dictionary. Since we store the dictionary entries lexically\nsorted, we can find a specific entry using binary search. First we\nlook up `Jim` in the middle of the dictionary, if the first entry in\nour middle block is bigger than `Jim` we scan through the block, if\nwe're bigger than every entry of the block, we search in the middle of\nthe second half of the dictionary. If we are less than `Jim` we can\nsearch in the first half. Wash-rinse-repeate we have access to our\nindex using an access mode that guarantees we get our answer in a log\nof the size of the dictionary `O(log(n))`.\n\nAnd going in *reverse* we can find the dictionary entry for a given\ninteger in `O(1)` (constant) time. We do this by keeping around an\nindex of block-offsets so we can quickly find which block we are\nin. The block offsets themselves are stored in a compressed data\nstructure which has constant time access (in our case a Log Array, but\nit could be another data structure such as an [Elias-Fano\nencoding](https://www.antoniomallia.it/sorted-integers-compression-with-elias-fano-encoding.html)).\n\nFor our graphs we have three dictionaries. One for nodes, which form\nthe Subjects/Objects, one for Predicates or edges and one for\nvalues.\n\nThe first shared dictionary allows us to use the same subject id for\nan object which is convenient in following chains. Predicates, or the\nedge name, is treated specially as it is seldom necessary to look them\nup in a chain, so they can reside in their own name space, can can use\nan overlapping range of integer ids with the nodes.\n\nWe also represent our *data* in a value dictionary. These are\nrepresented as a specific offset *above* the numbers used for nodes,\nwhich we add to the number of our dictionary entry to translate to and\nfrom id-space.\n\nNote, that to store data effectively here using a dictionary with\nfront encoding, it is important to store *everything* lexically. We'll\naddress lexical encodings of data types in another future blog.\n\n### The Adjacency List\n\nOnce we have our id, we need to see how we are connected. Let's\nimagine we are starting with `Joan` and our access mode is `(+,-,-)`.\n\nPerhaps there are only two people in our database for the moment, and\nour graph looks like this:\n\n```\nJim -address-> \"12 Mulberry Lane\"\nJim -dob-> \"1963-01-03\"\nJim -friend-> Jim\nJim -friend-> Joan\nJim -name-> \"Jim-Bob McGee\"\nJoan -address-> \"3 Builders street, house number 25, apartment number 12\"\nJoan -dob-> \"1985-03-12\"\nJoan -name-> \"Joan Doe\"\n```\n\nOur node dictionary is `{'Jim':1,'Joan':2}` our predicate dictionary\nis: `{address:1,dob:2,friend:3,name:4}` and our value dictionary is: `{'12\nMulberry Lane':1,'3 Builders street, house number 25, apartment number\n12':2,'1963-01-03':3,'1985-03-12':4,'Jim-Bob McGree':5,'Joan\nDoe':6}`. Note that we still have to apply an offset of 2 (the size of\nour node dictionary) to our value dictionary to get back and forth\nbetween id space and our dictionary entry number for values. Also, all\ndictionaries are 1 indexed rather than zero. We will treat zero as a\nspecial identifier representing emptiness.\n\nTo find out what predicates are associated with `Joan` we need to look\nthem up in an *adjacency list*. Internal to TerminusDB this is\nrepresented with a pair of data structures. A log-array and a\nbitindex. The log array stores the ids of our associated predicates,\nand the bitindex tells us which predicates are associated with which\nsubject.\n\nFor instance, to look up `Joan`, we first look up the second entry\n(Joan's id is 2). This is done with our succinct data structure, the\nbitindex.\n\n### The Conceptual structure\n\nOur bit index looks conceptually as follows:\n\n|Subjects| Subject Id | Bit Index | SP Array |\n|--------|------------|-----------|----------|\n| Jim    |  1         |  1        | 1        |\n|        |            |  1        | 2        |\n|        |            |  1        | 3        |\n|        |            |  0        | 4        |\n| Jane   |  2         |  1        | 1        |\n|        |            |  1        | 2        |\n|        |            |  0        | 4        |\n\nThe bit index tells us what subject is associated with which entry of\nthe SP Array (using *select*), and the SP Array holds the predicate id\nwhich is available to that subject. The *index* of the SP Array is\nconceptually an Subject-Pair identifier, refering to a concrete\nsubject (available by asking the *rank* of the bit-index, the\npopulation count of 1s at the specific offset) and simply by returning\nthe value of the SP Array at that index.\n\nMaking the *rank* and *select* operations fast for bit-indexes is its\nown subject which we will talk about in a moment. For now it's just\nimportant to know why we want them to be fast.\n\nWe have the following the concrete representations:\n\n```\nSP BitIndex: 1110110\nSP Array: [1,2,3,4,1,2,4]\n```\n\nWe can ask for the `(subject_id - 1)`th 0 of our bit-index to find out\nwhich section of the bit index we are interested in. So for instance,\nif we want to know what Jane is connected to, we look up the index of\nthe 1st 0, add one, and we find our subject-predicate starting\nindex. In this case that index is 5.\n\nWe can then iterate over this vector up until we reach the next 0 in\nour bit index, and these are all the SP pairs associated with Jane. In\nother words, the indexes into the SP array of 5 through 7 are\nassociated with the Subject 2, and have predicates 1, 2 and 4.\n\n```\n                 Jane's bits in the bit index\n                      | | |\nSP BitIndex:  1 1 1 0 1 1 0\nSP Array:    [1,2,3,4,1,2,4]\n                      | | |\n                      Jane's associated predicates:\n                        address (1), dob (2), name (4)\n                        ...but not friend(3)\n```\n\nNow, supposing we have an SP index, how do we find the object\nassociated? Since each predicate can point to multiple objects, we\nwill use the same trick again. In our case we only have one object for\neach element of the SP Array, Here, we use the Object Log Array.\n\n```\nSP BitIndex:  1 1 1 0 1 1 0\nSP Array:    [1,2,3,4,1,2,4]\nO BitIndex:   0 0 1 0 0 0 0 0\nObject Array [3,5,1,2,7,4,6,8]\n```\n\nOk, so what object is associated with Jane's name? First, we look up\nthe name to see that it is predicate id 4. We know that Jane starts at\nindex 5, so we can scan through the SP vector until we get a 4. We\nremember this index, which is 7.\n\nWe can now ask for the index of the (7-1=6)th zero in the Object\nBitIndex and add one to get 7. The number at this index is 8. Since `8 > 2`,\nthe max number for nodes, we know this is a value. We can subtract 2,\nget 6, and look up the number in the value dictionary, to retrieve\n\"Joan Doe\" (6). Success!\n\nNotice that there is *one* 1 in the Object BitIndex. This corresponds\nwith the fact that the predicate friend (3) has two elements in the\nObject Array. The 3 in the SP array corresponds with friend. It is at\nindex 3. When we look up the (3-1)th zero in the Object index we get a\n1, we add one to get 2, see that there is a 1 in this position, and\nknow that there is more than one value. We count the number of values\n(number of ones + 1), and get that there are two values. We can now\niterate in the Object arroun from the index 3 to the index 4.\n\nThe answers here are 1, and 2, both are less than or equal to our node\ncount, so we are nodes. We can look up the node names in our\ndictionary and find it is Jim, and Joan!\n\n### Reversal of fortune\n\nBut how do we go backward from object id to SP? We need another data\nstructure here. In this case it will be the inverted index of the\nObject Array.\n\n```\nO_SP BitIndex:  0 0 0 0 0 0 0 0\nO_SP Array     [2,2,0,5,1,4,3,6]\n```\n\nIf we know an object Id, for instance, we want to find the object\nassociated with the name \"Joan Doe\" we simply look it up in the Object\ndictionary, get an id of 6, add two (the offset for nodes) to\nget 8. Now we look up where we are in the Object to SP bit index, by\nasking for the (8-1)th 0, and add 1 and find that we are at SP index\nof 6. If we look up this index (6) in the SP_array, we find that it\ncorresponds with name (4), and we can count the number of zeros in the\nSP BitIndex up to this point to our Subject identifier which is 2, the\nid of Jane!\n\n### Bit Indexes\n\nOur bit index lets us perform two operations, `select` and `rank`. We\nwere using them above informally to find the ith zero\n(`my_array.select(0,i)`), and the population of zeros up to an index\n(`my_array.rank(0,i)`).\n\nOf course we could implement these operations trivially by scanning\nthrough a bit vector, but that would take time proportional to the\nnumber of elements `O(n)`. To do it faster we need to keep some\nadditional information.\n\nWe can see here that our implementation in rust keeps around some\nbook-keeping, and a `BitArray`, and each of blocks and sblocks is\nrepresented using a `LogArray`.\n\n```rust\npub struct BitIndex {\n    array: BitArray,\n    blocks: LogArray,\n    sblocks: LogArray,\n}\n```\n\nThe bit array just contains the bits themselves. The blocks however,\nkeep track of population counts so we know the rank, and the super\nblocks keep track of super population counts.\n\nFor instance, for the bit sequence used above, blocks of size 3 and\nsuper blocks of size 6, we might have:\n\n```\nBitArray:  1 1 1 0 1 1 0\nBlocks:   |  3  |  2  |  0  |\nSBlocks:  |     5     |     0    |\n```\n\nTo find the number of 1s at index j, we simply make the calculation of\nthe number of bits up to j in a block, and use the population counts\nfrom blocks and superblocks.  In rust pseudocode:\n\n```rust\nlet array = BitArray::from_bits(0b1110110);\nlet block_index = j / 3;\nlet sblock_index = block_index / 2;\nlet block_rank = blocks.sum_rank(blocks_index);\nlet sblock_rank = sblocks.sum_rank(sblock_index);\nlet bits_rank = array.sum_rank([block_index * 3],j);\nsbock_rank - block_rank + bits_rank\n```\n\nHere the `sum_rank` function just counts the rank as a sum from the\nlast superblock, last block or last bit sequence. This can be\nconsidered a constant time operation as each of these sizes is fixed\nand though we have to compute all three, three constant time\noperations is also constant.\n\nThe rank of index 4 is now calculated with `block_index = 1`,\n`sblock_index = 0`. Therefore we take 5, subtract 2, and add 1, to\nget 4. Indeed the correct rank. The number of zeros is the index + 1\nminus the population count of 1s, or 1.\n\nSo rank is done, but what about select? It turns out we can implement\nselect by doing a binary search with rank. Recall that select finds the\n`ith` zero or one. To implement this we can simply start from the mid point, look at the\nrank, and keep subdividing until we get a select that matches. Since\nrank is `O(n)`, this means that select, implemented in this way is `O(log(n))`.\n\n### Conclusion\n\nIf you've made it this far then congratulations, you've a lot of\nstamina! What you've learned so far is enough to make a graph with\nflexible query modes that can be used as a foundation for a database.\n\nAll of these succinct structures are tightly packed and therfore\noptimised to be *write once*, *read often*. They aren't well suited to\nmutable updates. In order to get these mutations, we'll need some more\nmachinery. In my next blog, we'll dig into how that works.\n", "date": "2023-01-13T19:46:13.033160", "feature": null}, {"@type": "Post", "title": "How to implement GraphQL in fourteen days", "content": "\nMy vision of TerminusDB has, since inception, been that it should be a\nflexible content platform. That's why we've tried to develop not only\nschema modelling capabilities but also git-for-data features, diff\ncalculators and automatically generated user interfaces.\n\nGiven our focus on content management and delivery, it might seem\nstrange then that it took us so long to start trying to position\nTerminusDB as an ideal platform for headless CMSes. It took our\ncommunity prodding us into it, but we're taking the plunge.\n\nTo be a content platform it helps if you can plug in to existing data\necosystems. And this has been a point of friction for TerminusDB.\n\nAnd that's why we decided to go ahead and implement GraphQL for\nTerminusDB. We (mostly Matthijs van Otterdijk with some help from\nmyself) implemented automatic schema construction from TerminusDB\nschemata and the associated Query engine in a little over a week.\n\nI'm extremely excited about this development, since it means that now\nTerminusDB will be available to many more languages (anything with a\nGraphQL client), there will be a high quality open-source method of\ndeveloping with GraphQL, and a straightforward method of exposing RDF\nas a GraphQL endpoint.\n\nIt feels like a lot of unnecessary frictions simply fall away, and\nTerminusDB seems to fit so naturally into the GraphQL environment that\nI'm confident it will quickly be one of the best platforms around for\nserving content via GraphQL.\n\n## Juniper\n\nWe didn't do it from scratch of course, we leveraged the already\nexisting, and very well designed (if not so brilliantly documented)\n[Juniper](https://github.com/graphql-rust/juniper).\n\nJuniper is really great and we can recommend it, but be wary of the\n[Junpier\nbook](https://graphql-rust.github.io/juniper/master/quickstart.html). It\nis pretty badly out of date. You're better off reading the code\nexamples and automatically generated rust docs.\n\nThis toolkit lets you develop GraphQL endpoints directly from your\nrust datastructures when you have a static schema, or to generate a\ndynamic schema yourself.\n\nWe used both approaches to expose different parts of TerminusDB.\n\n## Static - Exposing TerminusDB's innards\n\nThe internal aspects of TerminusDB, the system database (`_system`),\nthe repository graph (`_meta`) and the commit graph (`_commits`) are\nall available to explore using the GraphQL endpoint. This makes\nvarious administrative tasks simpler, and the powerful introspection\ntools that GraphQL provides, makes exploring this fairly straightfoward.\n\nYou can point any graphql client at a valid TerminusDB data product\nand you'll get access, not only to the data product, but this meta\ndata as well. For instance, you can look at the log of updates to the\ndatabase, or explore which branches exist.\n\n![TerminusDB Log Example](../assets/TerminusDB_GraphQL_Log.png)\n\n## Dynamic - Modelling TerminusDB Documents in GraphQL\n\nTerminusDB is build around the concept of a document.  In order to\nexpose this to GraphQL, we automatically create a number of *Object\ntype definitions* and *input Object type definitions* which enable the\nGraphQL user to explore content in a TerminusDB and filter and order\nit without any additional work.\n\nThe basic idea is that each TerminusDB document type yields an object\ntype, which has all the fields of the document type as retrieval\noperations. If a field points at another document type, this operation\nallows the retrieval of additiona linked documents, optionally\nfiltering and ordering on any of its properties. Each document type\nalso yields an operation in the top-level query object that similarly\nallows filtering and ordering.\n\nUsing the [Star Wars Schema](../assets/star-wars.json) schema, with\nthe [Star Wars Turtle](../assets/star-wars-terminusdb.ttl) loaded, we\ncan perform a query which searches all star ships, for the one called\n\"Millenium Falcon\", and then obtain the name of its pilot.\n\n```graphql\n{\n  Starship(label: \"Millennium Falcon\"){\n    label,\n    pilot{\n      label\n    }\n  }\n}\n```\n\nTo which we will get an answer such as the following:\n\n![Millenium Falcon Query Result](../assets/millenium_falcon.png)\n\nJuniper made adding these queries automatically from the schema\ndynamically fairly straightfoward. We simply traverse TerminusDB's\nschema and create acceptable Juniper GraphQL objects as a result.\n\nIn order to perform searches, We also use Juniper's resolution with\nlittle modification. We implemented parameters for these queries by\ndirectly connecting to TerminusDB's storage backend and created\nappropriate iterators in rust. The whole thing is pretty responsive\nand surprisingly robust for a couple of weeks of work!\n\n## The Future\n\nWe will be extending the GraphQL interface with additional features as\nwe go forward. We see this as the primary method of interaction for\nmany front-end applications which use TerminusDB so we want to make it\ncompletely painless.\n\nFirst we will extend the parameters for queries with a more elaborate\nfiltering system. We want to be able to filter your documents on the\nproperties of the documents which are linked to it as easily as the\ntop-level document.\n\nAdding an intermediate [pagination and\n-cursor](https://graphql.org/learn/pagination/)) object type is also\non our list. This will let us return cursors as well as metadata about\nqueries, such as the number of total results.\n\nWe also want to implement *back links* which will show you all things\nthat *link to* a given object, and not just what an object links to.\n\nWe would also like to introduce path queries directly to GraphQL. This\nwould let you specify a path expression which links you from the\ncurrent object, to any other object of interest. This will let you do\nshortest path type operations, or ordered path operations. This would\nreally make GraphQL much more *graphy*.\n\nFinally, we would like to be able to specify TerminusDB schemata in\nGraphQL's schema language. Since GraphQL's class system is strictly\nsimpler, we can embed a GraphQL schema in TerminusDB without loss of\ninformation.\n\nWe're interested in people playing around with this system and we're\nvery interested in feed back and suggestions.\n\nTo the Stars and Beyond!\n\n\n", "date": "2023-01-13T19:46:13.033160", "feature": {"@type": "Image", "location": "http://localhost:3000/assets/TerminusDB_GraphQL_Log.png", "alt": "TerminusDB Log Example"}}, {"@type": "Post", "title": "What's in a Name: URI Generation and Unique names for objects", "content": "## Fixing Linked-Data pain-points by briging RDF and RDBMS id generation\n\nIn TerminusDB we have both automatic and manual means of describing\nthe reference of a document in our graph. We have tried to make these\nas simple as possible to work with based on our experience of the\ndifficulties encountered trying to create URIs in RDF in practice.\n\nWe're going to look at various identifier generation strategies,\nincluding: manual, lexical, hash, random, and value hash.\n\nBut to see why we introduced these choices and strategies, it's useful\nto think a bit about ID references from first principles.\n\n## The Semantic Web\n\nOne of the really interesting ideas to come out of the Semantic Web\nwas the idea of using unique universal identifiers on a similar model\nto the URL. The Web forms a vasty interconnected graph, with edges\nspecified by these URLs.\n\nA *URI* is subtly different. A URI is a *Universal Resource Indicator*\nas opposed to a *Universal Resource Locator*. The distinction revolves\naround whether they should *resolve* to the resource that they\nindicate.\n\nA *URI* *could* be a *URL* but it need not be. A *URI* could stand in\nfor any sort of resource, physical or logical which we need to have a\nname for. It is, in a sense simply a name which we want to be\n*universal*.\n\nA good example of a URL that is also a URI, are the HTTPS syntax for\nDOI (Digital Object Identifiers) which specify a specific scientific\npaper. For instance, we can talk about the Quantum Tomography paper\n[Measured measurement](https://doi.org/10.1038/nphys1170) with the DOI\n`https://doi.org/10.1038/nphys1170`, but if you go to that address it\nwill forward you to the actual resource.\n\nAnother example where we might double up the meaning might be if we\nused the URL `https://en.wikipedia.org/wiki/Albert_Einstein` to refer\nto the person Albert Einstein in our database, but which will resolve\nautomatically to an article on wikipedia.\n\n## A thing and its Name\n\nHowever these URIs (or IRIs, for Internationalised Resource Indicator,\nif we are being international) do not have to know how to resolve to\nthe objects they describe, even if it can be handy for them to do\nso. If data is to move around, records will have to be in different\nstates in different places, so resolving an object to exactly what\ndata is associated with it, is not possible to do in all cases.\n\nDespite this, it might be nice to dereference a *canonical* version of\nthe data, for some notion of canonical. And so some thought should go\nin to how one might usefully make our URIs into URLs as well.\n\nAnother aspect of the subtly is the difference between the name of the\nthing and the thing itself. Of course the Albert Einstein URL above is\nnot actually yielding us Albert Einstein when we dereference it. It's\ngiving us some meta-data about Albert Einstein, since the resource\nitself is unavailable.\n\n## Universality\n\nThe universality of naming is a major benefit. It means that you have\na distributed means of describing the objects you are interested\nin. URIs can exist in multiple different database systems, or be\npassed around, live, in applications.\n\nThis is a big step forward over the use of situationaly dependent ids\nsuch as integers counting from zero, which is a very bad way of\nrepresenting things if you want to reference the system externally, or\nif you want to have a distributed or decentralised system.\n\n## How to Name Something?\n\nNaming is hard. Any programmer who has had to come up with a name for\na variable or function such that his colleagues, (or they themselves\nlater) can understand what it does knows this viscerally. When\nmultiple people or systems are trying to come up with names which\nagree with eachother, it's even worse.\n\nUnfortunately, there is no easy way to avoid this completely. Some\ncoordination and governance is required to obtain shared names\nappropriately. For URLs this is addressed with organizations such as\nICANN, but others who want to mange naming will have to produce their\nown approaches.\n\n## URI Shorthand\n\nIn terminusDB, we reference documents with a URI. This URI uses an\nimplicit or explicit *prefix* which is not necessary to refer to when\nthe context is unambiguous. You can think of this a bit like the way\nwe use modules for identifiers in programming languages.\n\nWhen I create a new data product (a new document graph) I specify a\nset of prefixes, including the default `@schema` and default `@base`\nprefixes. The schema prefixes are for the *type* or *schema* level,\nand the *instance* or *document* level.\n\nThese are specified in a context object. A typical context object will\nlook something like:\n\n```json\n{ \"@type\" : \"Context\",\n  \"@schema\" : \"https://lib.terminusdb.com/people#\",\n  \"@base\" : \"https://lib.terminusdb.com/people/\" }\n```\n\nWhen we refer to a specific element of the data collection, we would\nwrite down its short name as something like `Person/joe`. It's\nexpanded name would be something like:\n`https://lib.terminusdb.com/people/Person/joe`.\n\nUsing this sort of naming, all TerminusDB objects can be refered to\nwith an unambiguous URI, and we can also dump a TerminusDB database as\nvalid RDF.\n\n## Keys\n\nComing up with the right URI for an object is hard. Especially when\nyou are generating lots of them programattically. Fortunately, we\n*can* make things easier. We can use a *key*. Keys are used in\nrelational databases to establish the identity of a row, but they are\ngenerally treated as a constraint on the table. In fact we can use the\nkey as a unique name which can always be used to describe an object.\n\nSupposing we already have a unique identifier, such as a social\nsecurity number. In this case we can be assured that a record for an\nindividual person can have a uniquely designated object in our system.\n\nThe schema document for a `Person` class might look like this:\n\n```json\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Person\",\n  \"@key\" : { \"@type\" : \"Lexical\", \"@fields\" : [\"ssn\"]},\n  \"ssn\" : \"xsd:string\",\n  \"name\" : \"xsd:string\"\n}\n```\n\nUsing this key specification, an individual for the class would like\nsomething like:\n\n```json\n{ \"@type\" : \"Person\",\n  \"@id\" : \"Person/078-05-1120\",\n  \"ssn\" : \"078-05-1120\",\n  \"name\" : \"Hilda Schrader Whitcher\" }\n```\n\nIn fact, since we are using a key, we can simply leave out the `\"@id\"`\nfield when we are submitting and the field can be calculated\nautomatically. We could instead submit the following document:\n\n```json\n{ \"@type\" : \"Person\",\n  \"ssn\" : \"078-05-1120\",\n  \"name\" : \"Hilda Schrader Whitcher\" }\n```\n\nThis is handy, since we can ignore the process of ID generation\nentirely. We can submit updates without knowing what the current name\nof a document is, or understanding precisely its strategy for\ngeneration.\n\nThis particular case demonstrates a (famous!) leak of a social\nsecurity number. We reference a private, but unique identifier which\nshould not be displayed to everyone who might want to be able to\n*reference* such a document. In order to avoid the disclosure of\ninformation in the name, or if the name does not display very useful information we can use a *hash*.\n\n```json\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Person\",\n  \"@key\" : { \"@type\" : \"Hash\", \"@fields\" : [\"ssn\"]},\n  \"ssn\" : \"xsd:string\",\n  \"name\" : \"xsd:string\"\n}\n```\n\nWith this alternative naming strategy we have the id generated using sha256 as:\n\n`Person/ef6385e04468128770c86bf7e098c70fa7bbc1a50d81a071087f925283a4e7af`\n\nNow we have the same ability to generate the name uniquely, but this\ntime without leaking information the IRI.\n\nFor those from an RDF background, it might be interesting to see that\nthis consists of two graphs, one for the schema and one for instance,\nwith the following turtle:\n\nSchema:\n```turtle\n@base <terminusdb:///schema#> .\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix sys: <http://terminusdb.com/schema/sys#> .\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n@prefix doc: <data/> .\n\ndoc:Cons\\/2387e9641235cbfcc495f3deadb79d209a97b48d0233dbf015399eb74f8629fc\n  a rdf:List ;\n  rdf:first <schema#ssn> ;\n  rdf:rest rdf:nil .\n\n<schema#Person>\n  a sys:Class ;\n  sys:key <schema#Person/key/Hash/ssn> ;\n  <schema#name> xsd:string ;\n  <schema#ssn> xsd:string .\n\n<schema#Person/key/Hash/ssn>\n  a sys:Hash ;\n  sys:fields doc:Cons\\/2387e9641235cbfcc495f3deadb79d209a97b48d0233dbf015399eb74f8629fc .\n```\n\nInstance:\n```turtle\n@base <terminusdb:///data/> .\n@prefix scm: <../schema#> .\n<Person/ef6385e04468128770c86bf7e098c70fa7bbc1a50d81a071087f925283a4e7af>\n  a scm:Person ;\n  scm:name \"Hilda Schrader Whitcher\" ;\n  scm:ssn \"078-05-1120\" .\n```\n\n# When is a Thing not Another Thing\n\nKeeping the key in the name is a kind of content addressability. And\nin the case of the use of a hash, it is a variety of content\naddressable hashing.\n\nNotably, it does *not* disambguate the entire document. Usually we\nunderstand the *identity* of a thing to outlive its precise data. With\na fully content addressable hashing, a things identity is *precisely*\nthe information we have about it.\n\nOften you might want to change someones address, or even just add a\nnew address record with a new time span on it. It's likely that you\ndon't want either of these operations to change the *reference* to\nthis document.\n\nOn the other hand, you might want the *address* for the person to\nremain completely defined by its data. An address could have a fair\nbit of information in it, including the street, province, etc. So\ndefining this as a lexical could be done as something like:\n\n```json\n{ \"@type\" : \"Enum\",\n  \"@id\" : \"Country\",\n  \"@value\" : [\"US\", \"Ireland\", \"Austria\"] },\n\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Address\",\n  \"@key\" : { \"@type\" : \"Lexical\",\n             \"@fields\" : [\"street\", \"province\", \"country\"] }\n  \"street\" : \"xsd:string\",\n  \"province\" : \"xsd:string\",\n  \"country\" : \"Country\" }\n```\n\nThis will work, but perhaps it's easier to write:\n\n```json\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Address\",\n  \"@key\" : \"ValueHash\",\n  \"street\" : \"xsd:string\",\n  \"province\" : \"xsd:string\",\n  \"country\" : \"Country\" }\n```\n\nThe later designation will assign a hash address using *all* available\ndata. New attempts to add the old address will simply reuse the exact\nsame record!\n\n## Rolling the Dice\n\nWhen generating *events*, we need new identifiers every single\ntime. To do that we need to choose *fresh* identifiers which will not\noverlap with identifiers coming from others. In TerminusDB we do this\nwith the `Random` key generation strategy. These can be *refered* to\nfor update using their name in the same way as others, but when being\ncreated, they generate a large probabilistically unique hash. The hash\nis *very* large, so collision probabilities are astronomically low and\ncan be safely ignored.\n\nOur event schema might look as follows:\n\n```json\n{ \"@type\" : \"Enum\",\n  \"@values\" : [\"Volcanic Eruption\", \"Hell Fire\", \"Plague\"] }\n\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Event\",\n  \"event_type\" : \"EventType\",\n  \"actor\" : \"xsd:string\" }\n```\n\nWhen we insert some events (using the document interface), they might\nlook as follows:\n\n```json\n[{ \"@type\" : \"Event\",\n   \"event_type\" : \"Hell Fire\",\n   \"actor\" : \"God\" },\n { \"@type\" : \"Event\",\n   \"event_type\" : \"Volcanic Eruption\",\n   \"actor\" : \"Volcano\" },\n { \"@type\" : \"Event\",\n   \"event_type\" : \"Plague\",\n   \"actor\" : \"Locusts\" }]\n```\n\nThe API will return a list of document IDs:\n\n```json\n[\"Event/480d554f9357b974694a4ffc42f39b9ac38761bc28257cff14168ba18912c398\",\n \"Event/456691cbb7564edad50ffd4f4245f760b1d4eb4459a31761324180ef1fa75d50\",\n \"Event/ba629b086337d94c7ca89c02d04e6fa2e9cb28bbf03b7e93f30e74a9b5a7962c\"]\n```\n\nThese are just generated from whole cloth and have nothing to do with\ntheir data. Therefore, we will need to refer to them explicitly by ID\nfrom now on. This often makes the most sense when we will refer to\nthem from other (perhaps more explicitly named) objects.\n\n## Explict Naming\n\n> \u201cWhen I use a word,\u201d Humpty Dumpty said in rather a scornful tone,\n> \u201cit means just what I choose it to mean\u2014neither more nor less.\u201d\n\nSometimes we just need to use the name of a thing the way it\nexists. This could happen referring to documents with names that are\ngenerated externally, or it could be that we just want our URIs to be\nself-explanatory and not based on something which can be generated\nfrom a key.\n\nIn this case we just always pass the `@id` around whenever we refer to\nthe object, whether inserting, updating or deleting.\n\n## Easier is Better\n\nWe had a lot of pain trying to generate large RDF graphs when we were\ngetting started. The problem of naming, not having a good way to\nrecognise already entered objects, no good way of \"consing\" up a name\nfor something, whether tied to data, or random all caused stumbling\nblocks. A *lot* of special purpose logic went into scripts, where it\nwas made to be correct after a lot of tweaking, and then remained\ncompletely opaque to everyone who did not read the ingestion script.\n\nEmbedding id generation into our system has helped to alleviate a lot\nof those problems, and has made our system more declarative, and more\ntransparent. Hopefully with a bit of playing around, you can\nappreciate the advantages.\n", "date": "2023-01-13T19:46:13.033160", "feature": null}, {"@type": "Post", "title": "JSON as RDF", "content": "\nTerminusDB is a *document graph* database. Meaning that it stores JSON\ndocuments, and the *links* between them allowing you to use TerminusDB\nas both a document store and a graph database.\n\nThe ability to have *links* between documents requires that there be a\nschema which tells us how to interpret the JSON documents that we are\ndealing with. If we encounter a string which is in a position which is\nexpected to be a reference to another document, we try to resolve that\nreference, and we will throw an error if that thing does not exist.\n\nHowever in practice it is sometimes the case that you don't really\nwant to bother with a schema. You don't know precisely what the\nstructure is, and you don't need to maintain links. You just have a\nlump of JSON that you want to stick in the database. You may still\nwant to *search* for things in the database for certain fields\nhowever, so you would still like the data to be *indexed* as a graph,\nbut it is really a *tree* rather than a general graph with links.\n\nCan we find a way to represent such objects as RDF? If we can find a\nregular representation then we can create effective procedures for\nextraction and insertion of data in this format.\n\n## Representation\n\nYes, as it turns out, we can.\n\nLet's take the following document which we have borrowed from [json.org](https://json.org/example.html):\n\n```json\n{\"name\":\"John\", \"age\":30, \"car\":null}\n```\n\nLet's take each field and stick an arbitrary prefix on it. Let's call\nthis prefix `json`. Now we can imagine an elaboration of this document\nin RDF as the following:\n\n```turtle\njson:2b00042f7481c7b056c4b410d28f33cf a json:JSONDocument ;\n  json:name \"John\"^^xsd:string ;\n  json:age 30^^xsd:decimal ;\n  json:car \"null\"^^xsd:token .\n```\n\nSo first, what is this `json:2b00042f7481c7b056c4b410d28f33cf` that we\nchoose as the name of our document? We need to choose a name that will\nnot collide with others. One way to do this is to choose a random\ntoken to represent the document. This has the disadvantage that every\ntime we add the same document we get slightly different RDF.\n\nAnother method is to take a hash representation of the document. This\nhas the advantage that every time put a document in, it will be\nexactly in the same place. The *disadvantage* of this approach is that\nwe can end up with a DAG (Directed Acyclic Graph).\n\n## Directed Acyclicity\n\nIn some ways having a DAG is great. It means we get genuine structure\nsharing for our graphs and therefore use less space! However there are\nsome downsides as well. Let's look at an example *diamond* shaped DAG.\n\n```json\n { \"docname\" : \"a\",\n   \"b\" : { \"docname\" : \"b\",\n           \"d\" : { \"docname\" : \"d\" }},\n   \"c\" : { \"docname\" : \"c\",\n           \"d\" : { \"docname\" : \"d\" }}}\n```\n\nWe've given these documents a name field so we can better see what the\ngraph might look like. The downward links in these documents is as\nfollows:\n\n```\n     a\n    \u2199 \u2198\n   b   c\n    \u200c\u2198 \u2199\n     d\n```\n\nHere we can see that the document `d` is being shared between both `b`\nand `c`.\n\nLet's also imagine what the RDF for this might look like:\n\n```turtle\njson:190efe62bb06e500e676b4f2c596676d a json:JSONDocument ;\n  json:docname \"a\"^^xsd:string ;\n  json:b json:5a47fb2e88b7b0b165f2485b4ff01eb9 ;\n  json:c json:ecedab89c0c2b147f631f55d4a8f15c5 .\n\njson:5a47fb2e88b7b0b165f2485b4ff01eb9 a json:JSONDocument ;\n  json:docname \"b\"^^xsd:string ;\n  json:d json:972d07526bf7972abeaf77f51da84c8b .\n\njson:ecedab89c0c2b147f631f55d4a8f15c5 a json:JSONDocument ;\n  json:docname \"c\"^^xsd:string ;\n  json:d json:972d07526bf7972abeaf77f51da84c8b .\n\njson:972d07526bf7972abeaf77f51da84c8b a json:JSONDocument ;\n  json:docname \"d\"^^xsd:string .\n```\n\nBut what happens if we *delete* a document. Let's take\n`json:190efe62bb06e500e676b4f2c596676d` for instance. When we delete\nthis document we can't merely delete everything in the downward\nclosure. We have to check that we're not deleting anything that anyone\n*else* refers to. Since we can't see modifications that others might\nbe doing in the given transaction however, this means we *can't*\ndelete anything which has other references. In fact we need a *garbage\ncollection phase* which can transactionally remove everything which\nhas been made unreachable. But how do we know it is unreachable?\n\nSo probably we need an additional tag that tells us that we were\ninserted as a *subdocument* and then can assume that the identifiers are\nnot *really* supposed to be accessible.\n\n## Which to use?\n\nThe random approach and the hash approach share a similar document\ninterface. However the subtle differences matter when we're querying\nfor instance. If we look for random documents with a given field like\n`json:docname \"d\"^^xsd:string` for instance, in the one case get two\nanswers, in the other only one.\n\nThe ability to avoid having a garbage collection phase though means\nthat implementation of the random version is simpler, and it's not\nclear that the benefits of sharing are sufficiently great.\n\n## Datatypes\n\nThe datatypes of JSON need to have a one-to-one correspondence with\nRDF and XSD so we know how to marshall back and forth. I suggest the\nfollowing:\n\n\n| JSON   | XSD            |\n| :----: | :------------: |\n| number | `xsd:decimal`  |\n| string | `xsd:string`   |\n| bool   | `xsd:boolean`  |\n| null   | `xsd:token`    |\n| array  | `rdf:List `    |\n\n## Object type\n\nAnd how should we mark an object as being of the appropriate type\nhere? If we simply add a type specifier such as `sys:JSON` we could\nspecify a freely defined JSON as follows:\n\n```json\n{\"@type\":\"sys:JSON\", \"name\":\"John\", \"age\":30, \"car\":null}\n```\n\nAnd if we want to add a field in the schema as free JSON, we could do\nso as with the following schema:\n\n```json\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Thingy\",\n  \"name\" : \"xsd:string\",\n  \"metadata\" : \"sys:JSON\" }\n```\n\nNow an instance object for the following class might look like:\n\n```json\n{ \"@type\" : \"Thingy\",\n  \"name\" : \"A Thingy\",\n  \"metadata\" : { \"colour\" : \"green\",\n                 \"headlight_configuration\" : null }}\n```\n\n## Opaque JSON\n\nEven simpler than adding a graphified JSON representation would be a\ncompletely opaque and unindexed version which simply stored the above\nas a BLOB internally, but which parsed / unparsed the data when\nputting it into the database. This would be sufficient for many\nuse-cases but kills the graph-like and queryable nature. The main\nadvantage to this would be simplicity.\n\n## Conclusion\n\nI'll be trying to implement a prototype of this, so if anyone has\nstrong opinions, I'm open to suggestions!\n", "date": "2023-01-13T19:46:13.034160", "feature": null}, {"@type": "Post", "title": "What's the Difference: JSON diff and patch", "content": "\nWhat will the distributed data environment in Web3 look like?\n\nHow will we have a distributed network of data stores which allow\nupdates and synchronisations?\n\nWhat is it that allows git to perform distributed operations on text\nso effectively?\n\nIs it possible to do the same for structured data?\n\n## Web3\n\nThese questions are really at the heart of the *distributed* part of\nweb3. Web3 has other parts: immutability, cryptographic security,\netc. But these other elements do not answer how to perform\nupdates on distributed data stores.[*](#crdt)\n\nIn seeking the answer to these questions, I was led to see a rather\nsimple tool as foundational: JSON diff and patch.\n\nJSON, because JSON is the structured data format for the web.  This\nwill continue to be true for Web3. Everyone uses JSON for just about\neverything in our web architecture. Other formats are going to be\nincreasingly used as mere optimisations of JSON. Associative arrays\nhave the beauty of (*reasonable*) human readability, combined with\nwidespread native support in modern computer programming\nlanguages. Both computers and humans can read it, what's not to love!\n\nBut what about the diff and patch part?\n\n## The *use case* for diff and patch\n\nA fundamental tool in git's strategy for distributed management of\nsource code is the concept of the *diff* and the *patch*. These\nfoundational operations are what make git possible. *diff* is used to\nconstruct a *patch* which can be applied to an object such that the\nfinal state *makes sense* for some value of *makes sense*.\n\nThe application of patches happens because we want a certain *before*\nstate to be lifted to a certain *after* state. The patch doesn't\nspecify everything. Only what it expects to be true of the source, and\nwhat it expects to be true after the update.\n\nWith this it's possible to have distributed updates performed on\ndifferent parts of source text. Collisions result in some remedial\naction being required, but if there are no collisions everything can\nbe *merged* to obtain a final state which respects all updates, no\nmatter when or where they came from.\n\nThis is what allows git to be fully multi-master, without requiring or\nforcing synchronisation using any complex protocols (like RAFT).\n\n## Diff and patch in structured data\n\nDo similar situations arise with structured data?\n\nDefinitely.\n\nLet's imagine an object which stores information about items in our\nonline store.\n\n```javascript\n{ \"id\" : 13234,\n  \"name\" : \"Retro Encabulator Mark 2\",\n  \"description\" : \"The Retro Encabulator Mark II is the lastest \n                   development of the Retro Encabulator used to \n                   generate inverse reactive current for unilateral \n                   phase detractors.\"\n  \"category\" : \"Cardinal Grammeter Synchronisers\",\n  \"price\" : { \"value\" : \"3430.23\", \"currency\" : \"Euro\" }},\n  \"stock\" : 32,\n  \"suppliers\" : [\"Supplier/123\",\"Supplier/4332\"] }\n```\n\nIf Alice opens the object in an application and changes the name of\nthe item to \"Retro Encabulator Mark II\", it should be possible for Bob\nto update the suppliers list simultaneously without either stepping\non each others toes.\n\nIn applications this sort of curation operation is often achieved with\na *lock* on the object. Which means only one person can win. And locks\nare a massive source of pain, not only because you can't achieve\notherwise perfectly reasonable concurrent operations, but because you\nrisk getting stale locks and having to figure out when to release them.\n\nBut what if Sally didn't submit her whole object for update, but only\nthe part she wanted changed? And Bob did the same?\n\nNow we can perform the updates in three different places, locally for\nAlice, locally for Bob, and then finally at a shared server resource.\n\nThe structured patch could be determined by looking at the object\n*before* Alice submitted it, and after, using `diff`. The patch\nconstructed from Alice's diff might look like this:\n\n```javascript\n{ \"name\" : { \"@before\" : \"Retro Encabulator Mark 2\",\n             \"@after\" :  \"Retro Encabulator Mark II\"}}\n```\n\nAnd Bob's might look like:\n\n```javascript\n{ \"suppliers\" : { \"@before\" : [\"Supplier/123\",\"Supplier/4332\"],\n                  \"@after\" :  [\"Supplier/123\",\"Supplier/4332\",\n                               \"Supplier/385\"]}}\n```\n\nNow both can apply cleanly to the original document listed above. We\ncan stack either patch in any order without difficulty. Perhaps we ask\nBob and Alice to agree on the application order (using pull / push as\nis done with git). But maybe we just allow them to apply when they\narrive. The answer depends on the workflow.\n\n## Conflict\n\nBut what if Mary comes in before Alice and submits the following\npatch:\n\n```javascript\n{ \"name\" : { \"@before\" : \"Retro Encabulator Mark 2\",\n             \"@after\" :  \"Retro Encabulator Mark two\"}}\n```\n\nWe have a problem. But we see immediately that the two are in conflict\nand Alice can be asked to resolve the question by surfacing it. In the\ncase of data curation this is a perfectly reasonable workflow. And it\nis this problem of data curation that we can solve with the simplest\nversion of JSON diff.\n\nThis conflict can be surfaced to Alice, and Bob can be allowed to go\nabout his business. Could this particular problem be resolved in a\npurely automatic way with a CRDT? Definitely, but it probably will not\nresult in what you want. Last first will work of course, but then\nwhich is *more right* might need human review, and even worse it might\nresult in both results being interleaved (a likely outcome!).\n\nWe *could* make the before and after, however, be a text-based patch\nusing a textual diff. Probably gits line-based approach is *not* what\nwe want here, but rather one that takes words as atoms. It will not\nsolve this particular conflict, but it could make text fields much\nmore flexible.\n\nWhich of these you want, however, requires *semantic direction* of the\ndiff algorithm. While lots of structured diff problems will be solved\nby the simplest algorithm, ultimately we need to have a schema which\nhelps to direct the meaning of our diffs. String fields might be best\nline based, word based, or perhaps they must always be atomic (as with\nidentifiers).\n\n## Patch is simpler than Diff\n\nPatch is actually the simpler operation. Patch application basically\njust checks that the read state matches, and then substitutes the\nwrites.\n\nDiff by contrast has to calculate, and often in practice *guess* a\ngood transition from the read state to the write state. The specific\ntuning of the patch provided by a diff is dependent on the needs of\nthe application. There are *generic* algorithms that can work decently\nfor a range of applications, but there is no one size fits all. This\nis why we will need the *semantic direction* which can be provided by\na schema.\n\nDiff is also computationally *much* more expensive. Finding the\nminimal change means finding the maximal similarity. As it turns out,\nthis is pretty easy for the skeleton of a JSON dictionary, but rather\na pain for lists, and strings. And for lists of lists... Well, I'll\nget into that later.\n\nLet's just say it's no exaggeration that you can easily wander into\nthe heat-death of the universe. Hence heuristics have to be part of\nany fully automatic diff.\n\n## A Complex Patch gives rise to Distributed Transactions\n\nBut there are other workflows which might want slightly more flexible\napproach to ensuring data integrity. The *before* state is really\nsitting there to specify the *read object model*. It tells us what we\nwant to be true when we apply the patch.\n\nWith git this might be lines of text. For instance, to change a very\nsimple `README.txt` which initially says `hello world` to one that\nsays `hello squirrels`, git will produce a patch that looks something\nlike the following:\n\n```\nindex 3b18e51..3a9ea5d 100644\n--- a/README.txt\n+++ b/README.txt\n@@ -1 +1 @@\n-hello world\n+hello squirrels\n-- \n2.32.0\n```\n\nThis isn't the most compact patch, and it will conflict if hello were\nchanged to some other word, for instance `greetings` perhaps. The\nreason that it works well for git is that lines of text are a somewhat\nreasonable granularity for programming languages.\n\nBut the before and after don't have to be lines or words. The before\ncould be any specification of the read state. For a bank account\nwithdraw, we might ask for the before state to be larger than, or\nequal to the after state. This would be a nice little transaction for\nensuring we don't overdraw.\n\nOr perhaps we want the before state to be specified with a regex? Or\nmaybe we read a *lot* of values in order to calculate a further value\nin the object, in which case we want to know that *none* of these\nvalues change.\n\nThis approach gives us a kind of read isolation which is *tuned* to\nthe use-case we're actually working with. Making patch the unit of\nupdate gives us just the right granularity for our application, which\nreally can't be known in advance.\n\nThis is an advancement beyond the sort of isolation options usually\nprovided by a database, and one that extends naturally to objects or\ngraphs of interconnected objects (as exists in TerminusDB).\n\n## What we have and where we are going\n\nI've implemented a simple JSON diff and patch in TerminusX. But we're\nalso working on the extensions of this to those specified by a\nschema. It's also easy to implement and very interesting to imagine a\nfull space of patches, many of which could never be determined by a\ndiff, but which would be extremely handy to have for distributed\ntransactions over document stores. We will be adding these various\noperations as we run into use-cases in practice, but we're also very\nkeen to hear about use cases that people have already encountered in\nthe wild. Do let me know!\n\n<a name=\"crdt\">*</a> CRDTs answer this question for certain types of\ndata structures - but not for all. Only certain *types* of\ndata structures can be updated with these approaches. In addition, many\nupdates require human aided review and will never require a\nCRDT. Still others will have *object read model* conditions which can\nnot be specified in a CRDT. Ultimately our databases should support a\nrange of distributed datatypes including CRDT.\n", "date": "2023-01-13T19:46:13.034160", "feature": null}, {"@type": "Post", "title": "Many Worlds: a philosophy of data collaboration", "content": "\nData collaboration is a key driver of modern organisational\nsuccess. No aspect of modern life is untouched by the need and\nimportance of data collaboration. It is really expansive, covering how\nwe share and edit documents (whether that be Word, Excel, text or\nJSON), how we write databases and how we code.\n\nI would like to present a somewhat analytic philosophical take on what\ndata collaboration is and how we should think about it.  I believe\nthat this will help us not only understand what we are doing now, but\nwhere we need to go in the future to really achieve a more\n*enlightened* approach to data collaboration.\n\n## The Agreed Universe\n\nCollaboration requires having a shared *view* of the world. It\n*doesn't* require that we share absolutely *everything*. Coming to a\nshared concept of the state of something is what data collaboration is\nall about.\n\nTo do this, we need a bit of exploration as to where our assumptions\nline up with those of others. In order to collaborate we need ways to\nminimize the stomping-on of toes. Collaboration is therefore,\nnecessarily, a bit of a dance.\n\nThere are many ways to view this dance, but I think a special mention\nshould go to git.  *Git* really stands out as a tool for data\ncollaboration on one of the most complex types of data we have:\nCode. And the way we do this collaboration is very different from the\nway we collaborate with one of the other extremely widespread data\ncollabration tools: the database.\n\nNow there are various kinds of databases, various replication\ntechnologies, some very sophisticated and various levels of\n[isolation](https://en.wikipedia.org/wiki/Isolation_(database_systems))\ngiven which change the way we collaborate.\n\nIn order to think about *data* and *change*, we're going to stray into\na universe of multiple worlds which was perhaps best conceptualised by\n[Saul Kripke](https://en.wikipedia.org/wiki/Kripke_semantics) from\nwhom we will borrow liberally. This philosophical framework is very\ngeneral, but it can also be very precise. This makes it a useful lens\nthrough which to view our activities.\n\n## Linear Worlds\n\nThe very concept of isolation, a core concept in database systems, and\nthe I in ACID, stems from the notion that databases should have *one*\ncurrent state of the world. Our transactions exist to construct\n*movement* in this one world. Isolation allows us to ignore how others\nare changing the world and focus on our own problems. Since nobody\nlikes having to deal with others problems, this is good news.\n\nIt is really convenient when transactional commits happen with reads\nand writes that are scheduled relatively close to eachother and where\nscaling vertically is not an issue. It works so well that databases\nworking on this model are absolutely pervasive in our computer\narchitectures.\n\nEach database *query* tells us something about the state of the\ncurrent world. For instance, if we have a world `w` we can ask a\nquestion `parent(X,Y)` where we get all `X` `Y` for which\n`parent(X,Y)` is true.\n\n```\nDiagram:\n\nw\n\u22c6\n\nQuery:\n\nw \u22a2 parent(X,Y) \u2190 {[X = jim,  Y = jane ], [X = jim,  Y = joe],\n                   [X = kate, Y = elena], [X = kate, Y = tim]}\n```\n\nWe can read this as a query at the world `w`, which gives us back all\nsubstitutions of variables which would make the query true at world\n`w`.\n\nHere we have a world at which Jane and Joe are Jim's parents, and Tim\nand Elena are Kate's parent. That's sensible enough, but we may need\nto update this world when Jim and Kate have children.\n\nThis requires a *state transition*. We will go from a world `w` to `w'`\nvia some state transition `\u03c3` (sigma).\n\n```\nw   w'\n\u22c6 \u2192 \u22c6\n  \u03c3\n```\n\nLet's say `\u03c3` says that we are going to add a child Sarah of Jim and\nKate. We might write this as: `\u03c3 \u0323\u2261 insert:parent(sarah,jim) \u2227 insert:parent(sarah,kate)`.\n\nNow we can get a different answer at `w` and `w'` for a question such\nas the following:\n\n```\nw \u22a2 parent(sarah,Y) \u2190 {}\n\nw' \u22a2 parent(sarah,Y) \u2190 {[Y = jim], [Y = kate]}\n```\n\nDifferent worlds have different facts. And we move from one world to\nthe next through an arrow (which we could call an accessibility\nrelation). The arrow is *transitive*, in the sense that we can follow\nthe arrow through any number of hops.\n\nBut these worlds we have pictured above are arranged in a *linear*\nfashion. This is how we usually think of *our own* world. That is, we\ngenerally think of there being a single time-line, and everything that\nhappens is shared for all participants. As those with a bit of\nexperience with quantum mechanics may know, this may very well *not*\nbe true! However it is *mostly* true at human scales. And it is\ncertainly convenient to think in this fashion as it is simpler. And\nsimpler is better when it's still right enough. In the words of Albert\nEinstein:\n\n> A theory should be as simple as possible, and no simpler.\n\n## Locality in the Simulation\n\nWhen we try to simulate our understanding of the state of the world we\ninevitably find that we can't be everywhere at once. Locality is a\nfactor of the real world which is inescapable. At a physical level\nthis is because the speed of light provides an upper limit to our\ncommunication times.\n\nThe fundamental locality of operations is something which we must\nconstantly contend with in software engineering and systems design and\narchitecture. The difficulty of *cache coherence* is perhaps\nlegendary, and reflects this fact. Databases are no strangers to the\nproblem. But it also arises with \"real-time\" collaboration software\nlike google docs or google sheets.\n\n### Code\n\nThe way that we program with computer code is *step* orientated. In\ncompiled languages, we have to make a syntactically complete update, a\ncommit as it were, run the compiler and get an output. In dynamic\nlanguages like javascript and python we generally update the state of\nthe program code and re-run the code after a similarly syntactically\ncomplete change. Even in the now relatively rare *image* based\nprogramming models which were used in Lisp and SmallTalk for instance,\nupdates would happen to a chunk simultaneously - perhaps a function,\nclass definition or a procedure.\n\nThe naturality of this chunk-at-a-time transition is why git's commits\nare natural units for revision control. It also means that it is\nconvenient for our changes to be done in a *local* version which we\nedit in a single chunk, and only later attempt to reconcile with\nchanges which might be made by others.\n\nIt is *possible* to have simultaneous editing of code by multiple\nparticipants using other ideas such as\n[CRDTs](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type)\n(Conflict Free Replicated Data Type) or\n[OTs](https://en.wikipedia.org/wiki/Operational_transformation)\n(Operational Transformations) which we will look at in a bit (these also\ndeal with the problems of locality by the way), they simply aren't\nthat *useful* since we don't know when the code is ready to compile\nbecause the commit granularity of characters is too fine to make\nsense.\n\nHere it is useful to think of these commits as worlds. What is true at\na world is a state of a set of files. What we can query is the lines\nin these files. We call these worlds things like: `77a407c` or perhaps\nwe give them temporary names like `origin/main` to refer to the most\nrecent version. They are also distinctly *not* linear.\n\n```\n             main\n\u22c6 \u2192 \u22c6 \u2192 \u22c6 \u2192 \u22c6\n     \u2198\n       \u22c6 \u2192  \u22c6\n           dev\n\n```\n\nThis non-linearity leads us to branching worlds. And here is where git\ngets *really* interesting. We can *reconcile* histories by creating\nnew shared understandings through the process of [rebases]() and\n[merges]().\n\nEach state transition from one commit to another can be described as\nsome number of text line deletions and some number of line\nadditions. If these regions are non-overlapping we can perform a\n*three-way-merge*.\n\nThis new commit essentially makes the diagram *commute*. We can think\nof the new merge commit as arising from either of the two branches, as\na patch, both arriving at precisely the same final state.\n\n```\n\n                main\n\u22c6 \u2192 \u22c6 \u2192 \u22c6 \u2192 \u22c6 \u2192  \u22c6\n      \u2198       \u2197  \u21d1\n        \u22c6 \u2192 \u22c6    merge commit\n            dev\n```\n\nThis ability to get a shared world from divergent ones is what forms\nthe backbone of git's success in acting as a *collaboration* tool. We\ncollaborate on code by finding acceptable shared worlds after having\nmade state transitions whose granularities reflect the cadence of\nprogramm writing.\n\n### Replication\n\nReplication of databases means that we try to get local copies of a\ndatabase which are all the same, in some sense, and on some\ntimescale.\n\nOne simple approach uses a\n[primary](https://en.wikipedia.org/wiki/Replication_(computing)#Primary-backup_and_multi-primary_replication)\nfor transactions and potentially multiple *backups* which replicate\nthe latest state of this database (for some notion of latest). The\nstrategy here is to keep the *linear timeline* (which we saw above)\nwhich is to be organised by a single transaction processing server for\nsome transaction domain or shard. This tends to be much easier than\nhaving some sort of communication which would resolve issues with\ntransactions.\n\nHowever, more elaborate approaches which involve coming to\n[consensus](https://en.wikipedia.org/wiki/Consensus_(computer_science))\nalso exist. These make the timeline *seem* linear to the\nparticipants. But the secret underlying sauce in these algorithms is\nthat the timeline is *not* linear: We are actually dealing with\nmultiple worlds.\n\nOur task is to make sure that some agent processes can up with a way\nto arive at a shared final state which all participants agree\nwith. That is, the same final world state.\n\n```\n          (w\u2081 replicated)\n      w\u2080 \u2192 w\u2081  \u2192 w\u2090 \u2192 w\u2082\n             \u2198       \u2197\n               w\u2081 \u2192 w\u2091\n```\n\nThere are also very clever ways of relaxing how our worlds come to\nshared agreement. Instead of having to reduce immediately to `w\u2082`, we\ncan decide that our algorithm only needs to *eventually* get us\nthere. Intermediate reads, in different localities, will not get the\nsame world!\n\nSometimes this is good enough, and sometimes good enough is better\nbecause its faster. If you have a monotonically increasing counter for\ninstance, you don't care if you add one now, or add one later. The sum\nat the end will the same. People missing a bunch of up-votes when they\ncheck their social media will not cause serious concern. They'll see\nthem in a few hours and perhaps they will never be the wiser.\n\n\n```\n          (w\u2081 replicated)\n      w\u2080 \u2192 w\u2081  \u2192 w\u2090 \u2192 w\u2082 ...   w\u2093 (I eventually got joe's upvote)\n             \u2198               \u2197\n               w\u2081 \u2192 w\u2091 ... w\u2099\n```\n\n### CRDT and OT\n\nThe *illusion* of a common resource which is provided by google docs\nis a fantastic productivity tool. You can co-edit a document in\nreal-time and rarely does one think about *where* it is.\n\nBut it is actually somewhere! More correctly, it's multiple places at\nthe same time, in multiple different worlds with different states.\n\nIt is not a shared resource at all. Instead what we are doing is\ncreating replicas with a transaction processing system which can\n*re-order* transactions.\n\nWhen I edit a document I create some number of edit operations. These\nedit operations are applied to my local copy of a document. I then\nsend these to google's servers.\n\n```\nme:\n        w\u2080  \u2192   w\u2082  \u2192   w\u2099\n            \u03c3\u2080      \u03c3\u2081'\ngoogle:\n         (joe,\u03c3\u2081) (me,\u03c3\u2080)\njoe:\n        w\u2080  \u2192   w\u2081  \u2192   w\u2099\n            \u03c3\u2081     \u03c3\u2080'\n```\n\nGoogle sends on the updates to the clients allowing client updates to\nbe *fixed* by transforming them. Hence the name OT: Operational\nTransformation. We can get a linear world by taking google's view as\ncanonical, with the order of messages received. But each client can\nupdate their view independently after receiving the updates such that\nthey are appropriately transformed.\n\nAgain, we are finding a way to agree on our final world state - this\ntime by reordering transaction updates such that we don't have to\nagree them all in advance, which would make our application feel very\nlaggy and it would not have the illusion of being a shared resource at all!\n\nAnother way to achieve this same effect is with a CRDT. A CRDT[^crdt] builds\noperations which *commute in the first place*. That is, it doesn't\nmatter the order of the operations, when they are applied they arrive\nat the same final state. Of course this commutivity places a lot of\nrestrictions on *what* types of operations we can do. We certainly\ncan't have commuting bank transactions where you pay for your latte\nfrom your empty bank account and then get paid. But it can do a lot,\nand if you can make your `\u03c3`'s commute then life is really great.\n\n[^crdt]: CmRDT are based on commuting operations, but CvRDT use a commutative, associative and idemponent *merge* on states.\n\n## What Pieces are Missing\n\nI hope that seeing things laid out in a general framework which\nunifies these very disparate ways of collaborating has inspired some\nnew ideas. It certainly has got me thinking about what we *don't* have\nthat we probably *should* have. What pieces are missing from the\ncollaboration puzzle.\n\n### Structured Patches\n\nThe first is the concept of structured patches. This issue is very\n close to my heart as it is what we are currently working on at\n [TerminusDB](https://terminusdb.com), and I've written some\n preliminary thoughts about it in a discussion on [patches](https://github.com/terminusdb/terminusdb/discussions/686).\n\n\"There is nothing new under the sun\" applies here. There are several\nexcellent papers on the question of patches for structured data which\nI have pointed in my blog on [syntactic\nversioning](https://github.com/GavinMendelGleason/syntactic_versioning). There\nare example programmes which use these approaches which are open\nsource as well.\n\nHowever I think it is fair to say that the use of patches on\nstructured data has not hit prime-time. The tools to make use of it\nare not really there yet.\n\nBut perhaps more importantly, the scope of its power is not at all\nappreciated. It is a way to communicate information in a way which can\nmake explicit *when things commute*. That is, the conflicts which\narise during merges in git are caused by non-commutative patches. And\nthis is the same things as a transaction which does not commute.\n\nThis fact, now that we know a bit of Kripke semantics, should\nimmediately remind us of the kinds of things we do in other\ncircumstances when things do not commute!\n\n### Kripke Query\n\nThe other glaring hole in our current tech which becomes obvious when\nwe look at the states of the world as kripke structures is the ability\nto query through worlds.\n\nIn git we actually have a fair number of tools for this. `git log` is\nall about the worlds. Our git commands can *world-travel* which is\nactually a super-set of *time-travel*. We can go to different\nbranches, as well as different views of how the world evolved.\n\nBut git is a very limited sort of database. It essentially has chunks\nof text at worlds, with some associated metadata. With a real database\nwhich had the ability to travel through worlds, whole new avenues open\nup.\n\nOne of these is a *modal logic* query languages. Kripke semantics was\noriginally devised by Saul Kripke to create a semantics which could be\nused for modal logics. And one obvious modal logic which might be\nuseful for databases is temporal logic.\n\nWhat did we know at state `w`? This could be very important for\n*auditing*. What decisions were made at some point in the past relies\non what knowledge they had at that point. If you don't know what you\nknew, you can't evaluate if you did the right thing at that time. Of\ncourse this would seem to be an almost no-brainer for regulatory\nrequirements.\n\nThis is potentially really powerful. A database constraint is\ngenerally formulated as a proposition which obtains at every world. If\nwe know about our constraints as well as about our state transitions\n(patches) then we can know more about these.\n\nBut we can also potentially make constraints like *eventually* or\nother very powerful such statements such as are found in\n[CTL](https://en.wikipedia.org/wiki/Computation_tree_logic).\n\nSpeaking in more practical engineering terms, we might ask for when a\nparticular object was edited last, and by whom or what algorithm. Or\nwhen was the last time that a document refered to another document.\n\n### Hypothetical worlds\n\nWhen we are trying to resolve our updates to the world, sometimes it\nis convenient to build a thought experiment: what would this world\nlook like if some as yet untaken actions took place.\n\nHumans do this all the time with statements like, \"if you were to go\nto the store, would you get cheese, or biscuits or both?\". Note this\ndoesn't require that we go to the store. We instead try to resolve\nwhat would happen if we did.\n\nIn a computer however we can actually just try it out and see what\nhappens. We can then proceed to throw away the world if we don't like\nit.\n\nThis is already a routine phenonmena with github pull requests. github\nwill merge our pull request into a hypothetical commit and at this\ncommit we can resolve a number of propositions at the new world. These\npropositions might include linting, or unit tests or integration\ntests. All of these are *constraints* which we want to hold on the\nstate of the repository *after* commit. We run them to see if it works\nand then we can either have a *human* intervention (reviews or pushing\nthe merge button), or we can even merge automatically.\n\nWith structured data, this could prove a very powerful approach. We\ncan then easily externalise many difficult to encode constraints in\ncode which runs at the commit for instance, rather than try to do\neverything in *the one true query language* (tm).\n\n## A Different View\n\nThe decentralisation of data is simply a *fact* of locality in our\nuniverse. It has become fashionable in enterprise over the last twenty\nyears to attempt to suppress this fact through a combination of very\nimpressive technologies and organisational structures in approaches\nsuch as the data warehouse.\n\nAnd these technologies are amazing, useful and the *illusion* of\nlocality is fantastic when it can be made to work. Tricks like CRDTs\nand OT are super-cool.\n\nBut we're also missing multiple worlds of possibilities if we don't\npull back the curtains a bit and expose some more of the guts. The\nbeauty of git's model was in keeping all of our worlds visibile. We\ncan travel to the worlds, we can see the state transitions. This model\nenabled a host of really amazing things only one of which is\nversioning. It's real power came in enabling collaboration by exposing\nthe multiple worlds, their states and their transitions so that we\ncould work more directly with locality.\n", "date": "2023-01-13T19:46:13.034160", "feature": null}, {"@type": "Post", "title": "Mergeable Records: A Data Modelling Approach", "content": "## How to use Duplicate Detection to create Unique Entries.\n\nWhat happens when you have two records which are really meant to be\none?\n\nMost people with a cell-phone have encountered this problem with\nduplicate contacts. The problem of recognising them is tricky, but\nwhen you find the duplicate, you've got another problem: merging them.\n\n[Duplicate record detection](https://en.wikiversity.org/wiki/Duplicate_record_detection)\nis a very richly studied field with lots of techniques which are used\nto provide efficient methods of recognising duplicate entities in\nlarge data stores. I will not be going into that problem in detail\nhere (though I may describe how we do it at TerminusDB at a later date).\n\nInstead I want to look at the problem of merging them, once you have\nalready found them.\n\n## Provenance and Time scoping of Data Values\n\nIn the wild when we encounter data, we often have a *source* from\nwhich the data comes. Not only that, we have *different* information\nabout the same entities from different sources.\n\nI have encountered this problem in a very wide variety of entity\nmodelling problems in the wild, ranging from recording historical\ninformation, such as the population of Rome through to the subsidiary\nshare holdings of a company.\n\nIn both cases, the information is time-scoped. Rome's population is\nnot only uncertain, and being reported by a source, but it is\npurporting to do so for a specific time.\n\nSimilarly, when we have a record of a stock holding by a company, we\nhave it being purchased at a specific time. And of course, this\ninformation comes from somewhere. Either some provider of business\nintelligence, or perhaps scans of actual documents.\n\nAnd to make matters worse, not only is the information from a source,\nand time-scoped, but it is *also* inconsistent. It can disagree,\ndepending on what source it came from. And disagree it does. In\npractice we can have lots of sources disagree but the name of a\ncompany, it's address, the number of employees. And of course the\npopulation of Rome in 0AD might have more answers than the population of\nRome.\n\n\"surely there is some correct answer!\", you say.\n\nWell, that may be, but we may not be able to decide which it is, at\nleast not immediately. And luckily we don't have to, we can use all of\nthe answersq. Perhaps later we may want to have tests for accuracy of\na source, or perhaps we have other information that allows us to\nperform selection, and maybe we just make selections\nprobabilitistically!\n\nIn logic, being able to entertain multiple truths, simultaneously,\nwithout disappearing into a puff of smoke is known as [paraconsistent\nlogic](https://en.wikipedia.org/wiki/Paraconsistent_logic). We need a\nvariety of this with *provenence*, which records the origin of the\ndata. While obscure, in practice this a very fruitful way to model\nreal world problems.\n\n## A Simple Example: Company Records\n\nTo get a handle on how we might do this in practice, let's look at how\nwe might model a company in our schema.\n\n```json\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Company\",\n  \"name\" : \"xsd:string\",\n  \"employees\" : \"xsd:integer\"}\n```\n\nAnd a document which conforms to this schema might look like:\n\n```json\n{ \"@type\" : \"Company\",\n  \"@id\" : \"Company/TerminusDB\",\n  \"name\" : \"TerminusDB\",\n  \"employees\" : 17 }\n```\n\nThis is a very simple company record where we want to model the number\nof employees.\n\nAs we mentioned earlier, however, we need to know *when* this\ninformation happened and *why* we know it. Let's expand the company\nrecord a bit to give it a bit more variety.\n\n```json\n{ \"@type\" : \"Class\",\n  \"@id\" : \"TemporalScope\",\n  \"at\" : \"xsd:date\",\n}\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Source\",\n  \"name\" : \"xsd:string\",\n}\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Employees\",\n  \"@inherits\" : [\"TemporalScope\", \"Source\"],\n  \"@key\" : { \"@type\" : \"Lexical\",\n             \"@fields\" : [\"name\", \"value\", \"at\"] }\n  \"value\" : \"xsd:integer\"\n}\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Company\",\n  \"name\" : \"xsd:string\",\n  \"employees\" : { \"@type\" : \"Set\",\n                  \"@class\" : \"Employees\"}\n}\n```\n\nWell, things are a bit more complicated. We've changed `\"employees\"`\nto be a `\"Set\"`, which means we're going to allow any number of\n(unordered) `\"Employee\"` documents.\n\nWhat might one of the documents for this schema this look like?\n\n```\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Company/TerminusDB\",\n  \"name\" : \"TerminusDB\",\n  \"employees\" : [{ \"@type\" : \"Employees\",\n                   \"at\" : \"2022-03-01T17:44:52+01:00\",\n                   \"source\" : \"Gavin said so\",\n                   \"value\" : 17 }]\n}\n```\n\nWe have a source, a time at which it was reported, and a value, and\nthe employees value now lives in a set. This will be handy later. We\nalso establish an id for the `\"Employees\"` document by using a lexical\ncombination of the fields.\n\nOf course before we go on, we should note that in practice we would\n*also* want to change the name field to be like the `\"Employees\"`\nfield. Names of companies are not actually static in practice. They\nchange, and we have \"doing-business-as\", as well as slight differences\nof spelling etc.\n\n```json\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Name\",\n  \"@inherits\" : [\"TemporalScope\", \"Source\"],\n  \"@key\" : { \"@type\" : \"Lexical\",\n             \"@fields\" : [\"name\", \"value\", \"at\"] }\n  \"value\" : \"xsd:string\"\n}\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Company\",\n  \"name\" : { \"@type\" : \"Set\",\n              \"@class\" : \"Name\"}\n  \"employees\" : { \"@type\" : \"Set\",\n                  \"@class\" : \"Employees\"}\n}\n```\n\n## Merging Records\n\nNow that we have records scoped by their source and time, as well as\nthe fields pointing at *sets* of these scoped values, we can very\neasily merge records.\n\nSupposing we have the following two records which we have identified\nas being duplicates (perhaps by their company id in the company\nregistry which I have left out for simplicity).\n\n\n```json\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Company/TerminusDB\",\n  \"name\" : [{ \"@type\" : \"Employees\",\n              \"at\" : \"2022-03-01T17:44:52+01:00\",\n              \"source\" : \"Gavin said so\",\n              \"value\" : \"TerminusDB\" }],\n  \"employees\" : [{ \"@type\" : \"Employees\",\n                   \"at\" : \"2022-03-01T17:44:52+01:00\",\n                   \"source\" : \"Gavin said so\",\n                   \"value\" : 17 }]\n}\n```\n\nand\n\n```json\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Company/DataChemist\",\n  \"name\" : [{ \"@type\" : \"Employees\",\n              \"at\" : \"2020-01-01T20:24:32+00:00\",\n              \"source\" : \"Luke said so\",\n              \"value\" : \"DataChemist\" }],\n  \"employees\" : [{ \"@type\" : \"Employees\",\n                   \"at\" : \"2020-01-01T20:24:32+00:00\",\n                   \"source\" : \"Luke said so\",\n                   \"value\" : 5 }]\n}\n```\n\nNow a merger of the two records is as easy as literally appending the\nlists together.\n\n\n```json\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Company/TerminusDB\",\n  \"name\" : [{ \"@type\" : \"Employees\",\n              \"at\" : \"2022-03-01T17:44:52+01:00\",\n              \"source\" : \"Gavin said so\",\n              \"value\" : \"TerminusDB\" },\n            { \"@type\" : \"Employees\",\n              \"at\" : \"2020-01-01T20:24:32+00:00\",\n              \"source\" : \"Lukee said so\",\n              \"value\" : \"DataChemist\" }],\n  \"employees\" : [{ \"@type\" : \"Employees\",\n                   \"at\" : \"2022-03-01T17:44:52+01:00\",\n                   \"source\" : \"Gavin said so\",\n                   \"value\" : 17 },\n                 { \"@type\" : \"Employees\",\n                   \"at\" : \"2020-01-01T20:24:32+00:00\",\n                   \"source\" : \"Luke said so\",\n                   \"value\" : 5 }]\n}\n```\n\n## Dealing with Merged Records\n\nThis type of merger is both easy to write, a couple of lines of python\nfor instance, and gives us a *better* model of information. The\ndownside is complexity of the data and the queries. Queries now have\nto be more explicit about *what* we want to surface. But then what\n*should* such a record look like?\n\nYou *could* add a default name and employee field to the records, and\npromote one of the answers. Possibly the latest answer, possibly the\n*best* answer, maybe according to who is most reliable. Or perhaps\nthis is just a question of UI display, and we take the latest best\nrecord, or something which the user specifies in preferences.\n\nSo while we have made things a bit harder to look at, we've made it\nmatch what the data actually means more closely, and made it easier to\nupdate, and easier to enrich.\n\nThere are other things you might want to do, perhaps more structured\ninformation in the source information, or the like. But this is a good\nstarting point for modelling entity records in a wide variety of\nsituations in which we can expect to collate information from multiple\nsources.\n\n\n", "date": "2023-01-13T19:46:13.034160", "feature": null}, {"@type": "Post", "title": "What if MongoDB and Neo4j had a baby", "content": "\nThe NoSQL revolution has reshaped the world. Surely, some of these\nchanges are for worse, but most developers couldn't care less about what\nthe relational enthusiasts think.\n\nThe emergence of MongoDB brought the 0-to-60 for data storage in\napplications down by an order of magnitude. It's like a rocket sled\nfor application development (and probably just as safe). But developer\nexperience and ease of use are paramount, especially in a world\nover-burdened with complexity.\n\nNeo4j is no slouch either. Before Neo4j, graph databases were\nvirtually unknown. While it hasn't had the same impact of total\nreconfiguration of the data management landscape for applications, it\nhas found a healhty niche, largely in problem domains that one might\ndescribe as *embarrassingly connected*. And of course, once you start\nthinking in a graphy way, most problems end up looking *embarrassingly\nconnected*.\n\n## Data is JSON and the Graph is everywhere\n\nJSON is *the* medium of data communication. This is why MongoDB has\nhad such a profound effect on application development.  You can store\nyour data in essentially the way you mean to manipulated it in your\nprogramming language.  This radically reduces the impedence mismatch\nwhich has been a constant source of irritation for developers who were\nforced to use SQL. It's also re-usable as an abstract syntax tree for\nbuilding the communication of queries and updates themselves. No\nlonger do you have to build a query with brittle concatenations of\nstrings.\n\nBut trees are not really the only data structure we need even if they\nare the only thing that JSON can easily represent. Our programming\nlanguages have references or pointers because we need them to\nrepresent many of the data structures that we encounter. Although not\neverything is a graph, many things are. This is the insight which\ngraph databases bring. Relationships *between* data are almost as\nimportant as the data itself.\n\nBut why not both? Can't we have a database that allows us to fully\novercome the object-relational impedence mismatch of old? Which fuses\nthe benefits of the document store with the benefits of the graph?\n\nLuckily we can have our cake and eat it too. What we need is a\nlove-child of Mongo and Neo4j.\n\n## The Document Graph\n\nAll that is required to join these two worlds is the concept of a\nreference and a way to ensure that we have referential integrity\n(i.e. no dangling pointers). With this in hand we can design our\nstorage data structures such that we can follow these links\nefficiently.\n\nIn TerminusDB, we do this using URLs. This borrows from the original\nconcept of the HTML page, which is iself a structured document with\nhyper-links, but one designed for rendering rather than data\nmanipulation.\n\nInstead of HTML with URLs, we use JSON with URLs, but the concept is\nvery similar. As an example, a document which describes a person might\nlook something like:\n\n```javascript\n{ \"@id\" : \"Person/Jim+Smith\",\n  \"@type\" : \"Person\",\n  \"forename\" : \"Jim\",\n  \"surname\" : \"Smith\",\n  \"friends\" : [\"Person/Jill+Stone\",\"Person/Peter+Miller\"] }\n```\n\nWe write down the references relative to the base URL prefix which we\nassume for our collection, which might be something like\n`http://terminusdb.com/db/Terminators/Humans/`. The fully qualified\nURL would be rendered as something like:\n`http://terminusdb.com/db/Terminators/Humans/Person/Jim+Smith`. This\nmakes it easier to read and write. But how do we know this is a\nreference and not a string? This is an important distinction for\nseveral reasons. It tells us how to index our objects such that\ntraversals are fast, making it a real relationship rather than\nsomething that has to be calculated. It also keeps us from accidental\nmisinterpretation - disambiguating a URL from a database relationship\nfor instance. But it also allows us to ensure referential integrity,\nat least for links which are internal to our database. This is really\nimportant when dealing with large linked data stores, otherwise we\ncould easily end up with lots of broken links. It's very similar to a\nforeign key-constraint in a relational database.\n\nThese logical constraints are described with a schema. The one for a\nperson might be something like:\n\n```javascript\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Person\",\n  \"@key\" : { \"@type\" : \"Lexical\", \"@fields\" : [ \"forename\", \"surname\" ] },\n  \"forename\" : \"xsd:string\",\n  \"surname\" : \"xsd:string\",\n  \"friends\" : { \"@type\" : \"Set\", \"@class\" : \"Person\" }\n  }\n```\n\nThe use of JSON for a document database with hyperlinks gives us the\nbest of both worlds. The price we pay is that we have to be\nschema-first, something that is somewhat alien to both the MongoDB and\nGraphDB communities, but was common in the RDBMS era.\n\nThis cost is real but it *is* an advantage in the long-term for\nkeeping data integrity and avoiding the kind of speghetti that can\nresult from unconstrained graphs and documents. So you have to pay a\nbit in up-front capital costs, but the operational costs will be lower.\n\nSince the cost is real, we are always on the lookout for ways of\nreducing this upfront cost, including methods of inference with\nanomolie detection etc. This has the potential to get the best of all\nworlds, allowing us to do rapid prototyping and then subsequent\nlockdown of the schema.\n\n## From RDF to Linked Documents.\n\nTerminusDB started its life as an RDF database and that's still what\nit is under-the hood. RDF was the semantic web's answer to the\nquestion of how to represent data, leveraging the ideas which had been\nlearned in designing the web, and leveraging this for data. The\nsemantic web had already started delving into Web3 topics long before\nWeb3 existed as a concept.\n\nUnfortunately it never really took off. There are many reasons for\nthis, some of which are explored in [Graph Fundamentals Part 4: Linked\nData](https://terminusdb.com/blog/graph-fundamentals-part-4-linked-data/).\n\nPart of the reason for this is that RDF is somewhat hard to read, but\neven harder to write. Data is *completely* represented in the\nrelationships and not in documents (as it is with HTML). This provides\nno fundamental barrier to representation, but it can be a bit like a\npuzzle box to figure out how to weave everything into the graph, or\nwhat someone meant by their own particular weave once\nconstructed.[*](#wordnet).\n\nThis problem is alleviated by the concept of the *Document* which\nbundles links together into a single atomic collection of\ninformation. The schema gives us a map to move back and forth between\nthe JSON representation of the document, and the links in a graph.\n\nThe`Person` document for `Person/Jim+Smith` maps to something like:\n\n```turtle\n@prefix data: <http://terminusdb.com/db/Terminators/Humans/> .\n@prefix schema: <http://terminusdb.com/db/Terminators/Humans#> .\n\ndata:Person/Jim+Smith\n  a schema:Person ;\n  schema:forename : \"Jim\"^^xsd:string ;\n  schema:surname : \"Smith\"^^xsd:string ;\n  schema:friends : ( data:Person/Jill+Stone , data:Person/Peter+Miller )\n```\n\nAnd of course, this can be converted in the opposite direction. The\nJSON version has the advantage of familiarity and is immediately\nmanipulable in our favourite language.\n\n## Subdocuments\n\nIn addition if we want to have a larger fragment of the graph\nexpressed as a JSON document, we can simply use the concept of a\n*sub-document*. This is where we get signficant advantages over the\ndirect RDF representation, where what we intend to be a packaged\nobject is left as an exercise to the programmer.\n\nLet's extend the definition of a person above with an address.\n\n\n```javascript\n{ \"type\" : \"Enum\",\n  \"@id\" : \"Country\",\n  \"@value\" : [ \"Ireland\", \"South Africa\", \"UK\", \"Netherlands\",\n               \"Austria\", \"India\"] },\n\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Address\",\n  \"@subdocument\" : [],\n  \"@key\" : { \"@type\" : \"Random\"},\n  \"line1\" : \"xsd:string\",\n  \"line2\" : { \"@type\" : \"Optional\", \"@class\" : \"xsd:string\" },\n  \"postal_code\" : \"xsd:string\",\n  \"city\" : \"xsd:string\",\n  \"province\" { \"@type\" : \"Optional\", \"@class\" : \"xsd:string\" },\n  \"country\" : \"Country\" },\n\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Person\",\n  \"@key\" : { \"@type\" : \"Lexical\", \"@fields\" : [ \"forename\", \"surname\" ] },\n  \"forename\" : \"xsd:string\",\n  \"surname\" : \"xsd:string\",\n  \"address\" : \"Address\",\n  \"friends\" : { \"@type\" : \"Set\", \"@class\" : \"Person\" } }\n```\n\nNow we can include an address as part of our JSON document, and it is\nthought of as a single included component, even though we defined it\nseparately in a composable way in the schema.\n\nNow our record for Jim might be something like:\n\n```javascript\n{ \"@id\" : \"Person/Jim+Smith\",\n  \"@type\" : \"Person\",\n  \"forename\" : \"Jim\",\n  \"surname\" : \"Smith\",\n  \"friends\" : [\"Person/Jill+Stone\",\"Person/Peter+Miller\"],\n  \"address\" : { \"line1\" : \"Ferdinand Bordewijkstraat 1\",\n                \"city\" : \"Wageningen\",\n                \"province\" : \"Gelderland\",\n                \"postal_code\" : \"6708 RB\",\n                \"country\" : \"Netherlands\" } }\n```\n\nAnd further, when converting this to RDF we get a universally assigned\nand fixed ID - unlike the [blank\nnodes](https://en.wikipedia.org/wiki/Blank_node) of RDF. This is\nessentially a form of automatic\n[Skolemisation](https://en.wikipedia.org/wiki/Skolem_normal_form)\nwhich is user directed by describing how one would like the `@key` to\nbe constructed. We have chosen to generate the key from the name for\nthe `Person`, but have assigned a random `@key` for the address. We\ncould have instead used a `ValueHash` which would produce a unique\naddress based on the *content* (which makes this a form of content\naddressable hasing).\n\n## Elaboration and JSON-LD\n\nThe approach of a document oriented approach to RDF was already\npresent in [JSON-LD](https://en.wikipedia.org/wiki/JSON-LD), the\nRDF/Linked Data answer to JSON represntation.\n\nThis specification was well thought out and well designed. However in\nour experience in working with the representation we found it was *too\nlow level* to be convenient to write and use directly. It is instead\nused in TerminusDB largely as an intermediate representation, with the\nfully elaborated version converted into RDF to store in the Database.\n\nThe process of\n[elaboration](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.346&rep=rep1&type=pdf)\n(a concept borrowed from type theory) uses the schema to equip a\nfairly sparse JSON document with all of the information necessary to\nconvert it directly to RDF triples.  A fully elaborated version of the\n`Person` object might look like:\n\n```javascript\n{ \"@id\":\"data:Person/Jim+Smith\",\n  \"@type\":\"schema:Person\",\n  \"schema:forename\": { \"@value\":\"Jim\",\n                       \"@type\":\"xsd:string\" },\n  \"schema:surname\": { \"@value\":\"Smith\",\n                      \"@type\":\"xsd:string\" },\n  \"schema:friends\": {\"@container\":\"@set\",\n                     \"@type\":\"schema:Person\",\n                     \"@value\" : [ \"data:Person/Jill+Stone\",\n                                  \"data:Person/Peter+Miller\" ] }\n  }\n```\n\nThis elaboration could be useful as a means of communicating the\ndocument with much of its schematic information included. But for most\napplication development purposes, it simply is not needed.\n\nI'll be writing more about the elaboration process in a subsequent\npost.\n\n## Thesis - Antithesis - Synthesis\n\nThe use of subdocuments, the automatic generation of IDs, and the\ncomplete elimination of blank nodes is a major improvement over what\nexists already in the semantic web world.\n\nThe addition of native JSON document store capabilities as well as\nbeing a native graph database with fast link traversal means we have a\nsynthesis which leverages the advantages of both worlds. Documents and\ngraphs can live together naturally and compliment eachother.\n\nThere is work to be done to reduce the barrier to entry still futher,\nand eliminate some of the frictions of schema-first. However, we can\nalready see the advantages of what we have already in TerminusDB.\n\n<a name=\"wordnet\">*</a> To see a great example of this try sorting out\nhow to navigate the Lemmata in WordNet.\n", "date": "2023-01-13T19:46:13.034160", "feature": null}, {"@type": "Post", "title": "TerminusDB internals part 2: Change is gonna come", "content": "\nIn [Part 1 of TerminusDB internals](./graph_representation.md) we\nlooked at how to construct a graph using succinct data\nstructures. Succinct data structures are nice and compact, but they\nare not intrinsically *dynamic*. That is, you can't mutate them\ndirectly.\n\nTo see why it would be hard to mutate, imagine we have a dictonary:\n\n```\nJim\nJoan\n```\n\nAnd we want to add `Jack`.\n\n```\nJack\nJim\nJoan\n```\n\nThis entry shifts all of the indexes of everything by one. Every entry\nin all of our arrays which represent our graph is now wrong. Directly\nmutating the data structures is clearly not the easiest way to\nproceed.\n\n## Immutable updates\n\nThere is another strategy however. Instead of mutating, we can build\nup a *delta*. This *delta* is actually two new graphs. One which adds\nsome number of edges, and another which deletes them.\n\nThe engineering team calls these *positive and negative planes*, since\nit's easy to imagine them layering over the previous database. And the\noriginal plane is called the *base plane*, since it is only ever\npositive.\n\nThis trick is actually quite similar to the way that [Multi-Version\nConcurrency\nControl](https://en.wikipedia.org/wiki/Multiversion_concurrency_control)\nworks, and can be used to implement not only immutable updates, and\nversioning, but also concurrency control.\n\nHowever, now, when a query comes in, we need to do something a bit\nmore elaborate to resolve it. And what precisely we do depends on the\nmode.\n\nIf we are searching for a triple, with some aspects unknown, for\ninstance the mode `(+,-,-)`, we need to cascade downwards through our\nplanes searching for it.\n\nFor instance, the search for a triple:\n\n```javascript\nlet v = Vars(\"p\",\"x\");\ntriple(\"joe\", v.p, v.x)\n```\n\nIn a database which has experienced two updates\n\n\n```\n|      Plane 1             |        Plane 2           |     Plane 3           |\n| +(joe,name,\"Joe\")        | +(joe,dob,\"1978-01-01\")  | +(joe,name,\"Joe Bob\") |\n| +(joe,dob,\"1979-01-01\")  | -(joe,dob,\"1979-01-01\")  | -(joe,name,\"Joe\")     |\n```\n\nHere we will start at plane 3, fall back to plane 2, then the base\nplane, and then bubble back up.\n\n```\n|      Plane 1             |        Plane 2           |     Plane 3           |\n| +(joe,name,\"Joe\")        | +(joe,dob,\"1978-01-01\")  | +(joe,name,\"Joe Bob\") |\n| +(joe,dob,\"1979-01-01\")  | -(joe,dob,\"1979-01-01\")  | -(joe,name,\"Joe\")     |\n          ||                         ||                      ||\n          ||                         ||                      \\/\n          ||                         ||               (joe,name,\"Joe Bob\") =>Answer\n          ||                         \\/\n          ||                   (joe,dob,\"Joe Bob\")    ======================>Answer\n          \\/\n    (joe,name,\"Joe\")  ======================================> X\n    (joe,dob,\"1979-01-01\") ==========> X\n```\n\nThe two elements in the base plan get cancelled by deletions on the\nway up. They can't be answers since they aren't there anymore. This\napproach works for arbitrary modes, however, as the stack of planes\ngets large, it starts to get slow. The performance degrades linearly\nas a function of the number of transactions which have been performed\non the database. In practice you can often start to *feel* things\nslowing down when the stack is on the order of 10 transactions.\n\n## Delta Rollup\n\nHence we need to *rollup* some of these deltas. Essentially we need to\ndo a delta compression, create a new plane which represents some\nnumber of intermediate planes, but in which we've cancelled everything\nwhich was ephemeral (such as the two triples in the base plan).\n\nThis delta rollup, sits along side our previous planes, as an\n*equivalence* layer. All of the deltas are kept, as this allows us to\ntime-travel, and to push or pull commits to other servers, but we\nintroduce a pointer from our layer saying we have a faster way to\nquery. Should any query come in, they should preferentially take the\ndelta rollup layer instead.\n\nHowever, we don't need to *always* roll-up everything all of the\ntime. In addition, since the operation takes some time there is a\ndanger that rollups might occur too late to matter if expect to have a\nrollup at every layer.\n\nInstead we want to try to make sure that our rollups are *log-like*,\nin their occurance.  Basically we want to merge more things deeper\ninto the past, as these will be stable, and progressively fewer as we\nget closer to the present.\n\nFor instance, we might have the following rollups for some sequence of commits:\n\n\n```\n\n _____c1-4______  __c4-6_\n/               \\/       \\\nc1 - c2 - c3 - c4 - c5 - c6 - c7\n                              |\n                              main\n```\n\nHere we keep a rollup for the orders 4, then 2, then 1. As we go back\ninto the past the number of rollups gets smaller, and the total number\nof rollups we expect to see, and hence have to traverse is now\n*log-like* as the number of commits grows. For instance, as we\nincrease the number of commits we get:\n\n```\n\n   _______c1-8__________________\n  /                             \\\n _____c1-4______  __c4-6_        \\   ____c8-c11__\n/               \\/       \\        \\ /            \\\nc1 - c2 - c3 - c4 - c5 - c6 - c7 - c8 - c9 -c10 -c11 -c12\n                              |\n                              main\n```\n\nIn addition, because of our log-based approach, when rolling up the\nc1-8 commits we are only have to look at 4 layers, rather than 8. This\nspeeds up the merge operation on a running basis, and we will have a\nrelatively small number of layers to query, in fact in both cases we\nhave 3.\n\nIf you're paying close attention you might see there are potential\noptimisations in the representation of negatives. You could tomb-stone\na particular SP pair for instance, if there is cardinality zero, and\nsave some traversals. We have not implemented this yet in TerminusDB,\nbut we would like to do some experiments with it in the future.\n\n## Even the Graphs have Graphs\n\nSince we have this commit structure to our graph, access to a graph\ncomes in through the *head* of the graph. We need to know what is the\nmost recent current graph.\n\nIn TerminusDB, our graphs are schema checked, and our schemata\nthemselves are stored in a graph. So a typical data product is\nactually a number of graphs:\n\n* A Schema Graph\n* An Instance Graph\n* A Commit Graph\n* A Repo Graph\n\nIn addition we have a central graph, the *System Graph* which stores\nwhich data products exist and some properties about them that are\n*external* to the data product.\n\nThe combination of schema and instance graph are stored in the commit\ngraph. This associates a commit with both its current instance graph\nand its current schema graph. The *head* of a given branch points to\nthe most recent commit.\n\nWhen we open a data product for reading or writing, we look up the\nbranch in the commit graph, get the latest head, and then open the\nassociated layers for querying.\n\nThe Repo graph stores information about local and remote repositories,\nand which commit graphs are associated with them. This is used for the\npush/pull type repository collaboration features.\n\n## Transactions\n\nIn order to orchaestrate all of these graphs we need to be a bit\ncareful about the order in which we perform operations in order to\nmake sure that everything is properly atomic.\n\nThe operation order is something like this:\n\n1. We perform a query, and if there are any mutations we create a\n   *builder* recording all insertions and deletions.\n2. We update the commit graph, advancing the branch head, and update\n   the repo graph, advancing the repo head to point to the commit\n   graph id, creating builders for them as well.\n3. Once the query has completed successfully, we transform each of these\n   builders into a *validation object*. A validation object checks any\n   outstanding schema checks which could not be quickly performed\n   during the query. For instance referential integrity checking which\n   requires that we have everything completed before we can check that\n   it holds.\n4. We write all of the validated objects to disk, synchronising on\n   completion of all layers.\n3. We set the label file of the database to point at the most advanced\n   repository identifier.\n\nIf someone has gotten in and changed the label file before us, we use\noptimistic concurrency and re-execute the query. This gives us our\nacid properties.\n\n## The Journey Forward\n\nOne might notice however that this is a bit too strict. It would be\nnice if we could check to see if some weaker form of isolation could\nbe maintained so we don't have to re-run the query. Instead we might\nbe able to simply place our current transaction on top of the previous\none without running anything again.\n\nIn order to do this we need a concept of an isolation level, for\ninstance snap-shot isolation were we could maintain a read-set which\nspecifies which things must be stable based on what we read during the\nquery.\n\nWe could also be a bit more clever about the way we deal with garbage\ncreated during transactions. Currently we leave around layers that\nwe've written even when the transaction fails. This can be cleaned up\nwith a garbage collection process, but it's probably better if we\ndon't write it in the first place.\n\nWe are keen to get these improvements into a future release of\nTerminusDB.\n\nIn the next blog on TerminusDB internals, we'll look at data layout,\nsearch and compression. If you've made it to the bottom of this post,\nthen you're probably waiting for this next one with bated breath.\n", "date": "2023-01-13T19:46:13.034160", "feature": null}, {"@type": "Post", "title": "The Semantic Web is Dead - Long Live the Semantic Web!", "content": "\n[The Semantic Web](https://en.wikipedia.org/wiki/Semantic_Web) is\nrarely mentioned these days, so seldom that the declaration of its\ndeath could be met by most of a younger generation of programmers with\na question: \"The Semantic Who?\"\n\n![Semantic web interest decreasing over time](../assets/semantic_web_over_time.png)\n\nThis change in status is significant, but in some ways the Semantic\nWeb was on life-support since inception, and it continued to survive\nonly with the medical intervention of academic departments who had no\nneed to produce useable software or solve serious industry needs.\n\nThat's not to say that Semantic Web technologies *never* served any\nindustry needs. They certainly did so, but their penetration was\nlimited. And this limited penetration was not only the result of\nignorance on the part of data architects or software engineers. It was\nalso the fault of deep problems with the ideas in the\nSemantic Web itself.\n\n## Why the Semantic Web is a Great Idea\n\nThe Semantic Web's demise is a tragedy, because we need the Semantic\nWeb. But as with all things in life, one must adapt or die, and the\nadaptations of the Semantic Web were late, ill advised, and did not\naddress the core problems which led it to be the wrong approach in the\nfirst place.\n\nWhy do we need the Semantic Web?\n\nBecause distributed, interoperable, well defined data is literally the\nmost *central* problem for the current and near future human\neconomy. Knowledge is power, and distributable, actionable knowledge,\ncreates opportunities and efficiencies impossible with out it.\n\nWe need this. And we do not have this. Yet.\n\nThe Semantic Web set out ambitiously to solve this problem, but the\nproblem remains. My experience in engineering is that you almost\nalways get things wrong the first time, then you take that experience\nforward and fix the core problems. Eventually you might get something\nthat really sticks.\n\nThe analogy of the World Wide Web here is also deeply suggestive of\nwhat *might* be possible.  We have an incredible world wide system of\ndocument storage and retrieval, the World Wide Web. These documents are\nintended to be presented and (since Web 2.0) interacted with. But the\nproblem of getting machines to talk to eachother about the most basic\nrecords in a sensible way is still open.\n\nThe Semantic Web was going to fix this. We would have not only\nstructured documents, but structured data. We would be able to\ndescribe not only records, but the *meaning* of records. The records\nwould allow machines, which could be given access to *meaning* to\ninfer consequences from the data. We were going to have a rich distributed\ndata environment that would get richer with contributions from people\nall over the world.\n\nIt would be like Wikipedia, but even more all encompassing, and far\nmore transformational. The effect of the Weather in Ireland on cow\nprices, would be directly accessible and live, and could be\nused to compute the likely costs of a steak at the super market. Live\nfeeds of wind data could be accessed to provide continental balancing\noperations for the grid.\n\nIn short, the information age needs an information super-highway and\nnot just a big pipe for interactive documents.\n\n## Key Innovations\n\nThe core ideas and aims of the Semantic Web were largely correct.\n\nWe should use a very flexible data structure, and a graph is very\nflexible. Every data model which is not computation can fit in a\ngraph, and abstract syntax trees can easily represent intended\ncomputations.\n\nThe ability to reference data resources means that we need large\nunambiguous identifiers. URIs are able to solve this problem (though\nthere is still much ambiguity about the *meaning* of the URI and its\nrelationship to dereferencing).\n\nWe need a rich language to describe what the content *is* and what it\nmeans. Communicating information about our information is as important\nas the information itself.\n\n## Where it all went wrong\n\nBut from here on out, the Semantic Web begins to make some serious\nerrors that basically made it impossible to gain wide-spread\nacceptance. And while things are improving inside of Semantic Web\nsilos, interest is still waning.\n\n### The Format\n\nThe first problem is really one that is hard to see without 20/20\nhindsight. Triples can be used to describe a labelled graph. That is,\nwe can have three parts, S, P, and O and use this to denote the\norigin, labelled edge, and target respectively as is done with\n[N-Triples](https://www.w3.org/TR/n-triples/). It might look something like:\n\n```n-triples\n<http://example.com/a> <http://example.com#p> <http://example.com/b>\n<http://example.com/a> <http://example.com#name> \"a\"^^<http://www.w3.org/2001/XMLSchema#string>\n<http://example.com/b> <http://example.com#q> <http://example.com/a>\n<http://example.com/b http://example.com#name> \"b\"^^<http://www.w3.org/2001/XMLSchema#string>\n```\n\nThis is great because we are representing our graph with long names,\npresumably within a context where *our* definitions are under our\ncontrol. This is important if we want a large world of\ndistributed data. We even have some terminals in our graph with *data*\nwhich can allow us to representing typical datatypes which we do not\nwant to weave out of the graph individually (Peano arithemetic would\nbe a bit much).\n\nAnd if we're lucky, we can make it possible to disclose information\nabout the meaning by attempts to *dereference* them using the given\nprotocol. Very meta.\n\nThis graph above represents a simple loop, but as we can see, is a bit\nhard for a human to read. We can fix this by defining some prefixes\n`ex:` to mean `http://example.org/stuff/` and `exs:` to mean\n`http://example.com/stuff/schema#` and `xsd:` to mean\n`http://www.w3.org/2001/XMLSchema#`.\n\n```turtle\nex:a exs:p ex:b .\nex:a exs:name \"a\"^^xsd:string .\nex:b exs:q ex:a .\nex:b exs:name \"b\"^^xsd:string .\n```\n\nThat's certainly a bit better. However, redundancy is not our friend\nhere. It's easier again to read using the Turtle format, which allows\nus to refer to prior lines of information as a short hand. This is good\nfor human readability, but also for computers, which have to read less\ninformation (which becomes an issue when graphs are huge).\n\n```turtle\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n@prefix ex: <http://example.org/stuff/> .\n@prefix exs: <http://example.org/stuff/schema#> .\n\nex:a exs:p ex:b ;\n     exs:name \"a\"^^xsd:string .\nex:b exs:q ex:a ;\n     exs:name \"b\"^^xsd:string .\n```\n\nSo far so good. We now have a way\n([Turtle](https://www.w3.org/TR/turtle/)) to represent graphs which\ncan be communicated. Unfortunately few programming tools have any way\nof reading this into an easily manipulable data object. XML was a\ncontender, and XML indeed can represent these graphs in an alternative\nsyntax. But XML, despite vastly larger uptake by industry, is falling\ninto disuse itself because it is verbose, hard to read, and similarly\nhard to manipulate.\n\nWhile all of this was taking place,\n[JSON](https://www.json.org/json-en.html) was becaming the defacto\ndata interchange standard. Many in the Semantic Web community scoff at\nJSON and vocally claim it is a mistake. But Vox populi vox Dei.\n\nJSON and the related [YAML](https://yaml.org/) are among the best data\ninterchange formats. They are fairly easily read by humans, they map\ndirectly to vastly popular data structures which are ubiquitious now in\nprogramming languages (the associative array) and are extremely well\nsupported for lookup, iteration, pretty printing, and update.\n\nXML by contrast is awful to look at. And neither is it very fun to\nprocess XML documents while using a programming language (although\n[prolog](https://www.swi-prolog.org/pldoc/man?predicate=xpath/3)\nactually fits pretty naturally on XML as a query language).\n\nIn an alternative world were Lisp won before the AI winter, we might\nhave ended up with\n[S-Expressions](https://en.wikipedia.org/wiki/S-expression), but even\nthese are probably worse as they do not make naming of the keys\nmandatory (as opposed to by-position arguments), leading to less\nreadability without having more context.\n\nI'm absolutely positive that some people reading this are going to\ncomplain that syntax is just syntax, how things are serialised is\nsuperfluous and that a good library can make it easy to use INSERT MY\nTHING HERE. But these people are wrong and their opinions will drift\ninto obscurity the same way that Turtle and XML will. Naming is deeply\nimportant, it is no mere triviality; so how much more important is\nsentence structure?\n\nOf course there is [JSON-LD](https://json-ld.org/). This was a very\nserious attempt to bring Linked Data into the JSON developments, and\nshould be applauded. It is however, needlessly verbose as an\ninterchange format. I'll explain more later.\n\nJSON, and relatives are still expanding in use and will continue to do\nso. The Semantic Web of the Future must embrace this fact.\n\n### The Logic\n\nThere are many different and overlapping standards which define\nSemantic Web technologies, but I think we can focus on\n[OWL](https://en.wikipedia.org/wiki/Web_Ontology_Language) which\nrepresents one of the most serious attempts to create a formal basis\nfor the description of data.\n\nThe idea of representing the structure of data in a data structure\nthat is the same as the data is of course not a new idea (Think S-Exp)\nbut it is a good one. And OWL took this up. OWL is defined to use the\nsame kind of turtle format as above.\n\nThe problem with OWL is that it solved a problem, but not one that\nanyone wanted solved. It also did it in a very hard to use way. So\nhard to use that almost nobody has successfully used it. I've tried.\nAnd I've written many machine checked proofs in everything ranging\nfrom [Atelier B](https://www.atelierb.eu/en/atelier-b-tools/), through\n[FDR4](https://cocotec.io/fdr/) to [Agda](https://github.com/agda/agda)\nand [Coq](https://coq.inria.fr/).\n\nMaybe I'm not the sharpest knife in the drawer, but I've written\nsignificant proofs in these diverse languages, and never experienced\nthe same pain as with OWL.\n\nWhat is OWL *supposed* to do. It's supposed to be an\n[Ontology](https://en.wikipedia.org/wiki/Ontology) language. That is,\na language that helps us reasons about the categories of things, and\nhow they inter-relate. It is closely related to [Description\nLogics](https://en.wikipedia.org/wiki/Description_logic) used in\nseveral specialised areas in which the codification is both complex\nand very important such as biomedical knowledge.\n\nIt however makes some fatal errors that impede its use.\n\nTypes are easier for programmers to work with than these logical\nlanguages. Types are important, not just in low level and compiled\nlanguages, but increasingly in dynamic languages such as python and\njavascript. Speed is just one benefit of telling the computer what you\nmean. Another benefit is that the computer can tell you if what you\nmean is garbage.\n\nAnd this is why OWL is such a bear. It is very easy to tell OWL you\nmean something and it just believes you. It provides very little\nsupport to the data architect in making sure that things make sense.\n\nFor instance, in a programming language, if I define a class A as a\nsubclass of class B, and then define a class B as a subclass of class\nA, it will most likely complain. In OWL it will just equate class A\nwith class B.\n\nThis sort of equivocation by circularity is virtually *never* what one\nmeans. Yet I found numerous examples of these cycles in wild\nOWL. Nobody noticed them because OWL really had no problem with them,\nand it is not unless you actually check what inferences arise that you\ncan see the problem.\n\nAnd this is not unusual. One of the formalisers of OWL said this:\n\n![Inconsistencies produce nonsense more than obvious contradictions, and often when long inference chains are followed. \\@ImageSnippets found one in Wikidata that implied that native Americans are insects, another that Paris is an \"abstract media type\". I'm sure there are more.](../assets/pat_hayes_twitter.png)\n\nThe logical approach simply gives the practitioner too much rope to\nhang themselves, with very little feedback. An advanced researcher can\ndiscover these logical conundra but it is too much mental burden for\nevery day use.\n\nWhat caused this problem to arise, and what do we do about it?\n\n### Lets make everything *potentially* the same\n\nFirst, not having a [Unique name\nassumption](https://en.wikipedia.org/wiki/Unique_name_assumption) is a\nmistake. Plain and simple. This is the idea that any two URIs might\nactually mean the same thing unless we explicitly say otherwise. This\nis a terrible idea for computers in much the same way as it is for\nhumans attempting knowledge representation.\n\nIf you want something to be the same you should be forced to say\nit. Aliasing is a nightmare, computationally expensive and leads to\ndifficult to understand reasoning.\n\nThe second mistake serious mistake is the Open World Assumption, the\nconverse of the [Closed World\nAssumption](https://en.wikipedia.org/wiki/Closed-world_assumption). This\nassumes that there is as yet ungathered data, forcing us to reason\nonly about what we have at the minute. This might sound like a clever\nproposal but in practice makes reasoning weak.\n\nBasic things like checking that we have exactly one name for someone\nbecome stupidly complicated with the overlap of the above two\nrules. OWL would rather make two people the same, then complain that\nyou've done something wrong.\n\n### Things are too Far Apart\n\nIt may seem banal and \"just syntax\", but when writing large\nOntologies, OWL makes you wander far from home when trying to define\nthe meaning of a class. The class requires lots of separately defined\nproperties, each of which is verbose. When you add restrictions to them\nit becomes even worse: you have to add yet another class that you then\nintersect with your class to ensure the restriction.\n\n```turtle\nsystem:Capability\n  a owl:Class ;\n  rdfs:subClassOf system:Document, system:CapabilityScopeRestriction ;\n  rdfs:label \"Capability\"@en ;\n  rdfs:comment \"A capability confers access to a database or server action\"@en .\n\nsystem:capability\n  a owl:ObjectProperty ;\n  rdfs:label \"capability\"@en ;\n  rdfs:comment \"associates a role with its capabilities\"@en ;\n  rdfs:domain system:Role ;\n  rdfs:range system:Capability .\n\nsystem:capability_scope\n  a owl:ObjectProperty ;\n  rdfs:label \"Capability Scope\"@en ;\n  rdfs:comment \"The scope of a capability - the resource that the capability applies to\"@en ;\n  rdfs:domain system:Capability ;\n  rdfs:range system:Resource .\n```\n\nThis may be expressive but extremely annoying and time consuming. In\npractice I've seen people completely skip ontologising because the\npain barrier was so high specifically for this reason. Making routine\ntasks time consuming is a great way to ensure they don't happen.\n\n### Polluting the Data with Inference\n\nWhile you can use OWL to talk about your data model, you can also use\nit to enrich your data from the model. This sounds brilliant because\nwe often want calculated information. However, in practice this is\nnever so nice as a [view](https://en.wikipedia.org/wiki/View_(SQL)) is\nin SQL. Why?\n\nThe reason is that with a view you create a new distinct resource\nwhich is calculated. With clever technology you can update the data\nand get a new view updated automatically. Some advanced databases even\nhave ways to insert into a view to update the original tables.\n\nIn OWL we entail things into the graph which then look just like the\ndata from which it was entailed. This makes life complicated when\ntrying to distinguish from information you got from the real world,\nand information that you got from some chain of reasoning.\n\nAnd chains of reasoning can be very fraught, pollute the database. In\npractice I often found that things were performed as a batch load\nprocess to avoid the problem by restarting with new data. This is of\ncourse a ridiculous way to work with a large database.\n\nThe dream of computers which reasoned about data as it arrived to\ncreate some great symbolic AI, without a\n[Doxastic](https://en.wikipedia.org/wiki/Doxastic_logic) approach is\nfrankly silly.\n\n### Not much in the way of useable constraints, sorry\n\nAnd we can't use OWL to provide effective constraints over data. Open\nWorld precludes this. We can sometimes say when things are wrong, but\nwe are never sure if things are right. We might always have one more\nelement of a cardinality check that we just don't know yet.\n\nPeople who really wanted to use the Semantic Web in industry got\nfairly fed up with this difficulty and writing individual programmatic\ncheckers and SPARQL queries to patch up the deficiency and eventually\ninvented [SHACL](https://www.w3.org/TR/shacl/).\n\nSHACL goes a long way to alleviating this problem with OWL, but then\nit doesn't replace OWL. Instead of giving us a schematic ontology\nlanguage, we are left with two different languages which must now be\nkept in sync. Or we ditch the more descriptive partner, OWL and just\nuse SHACL and move RDF data closer to what is already done in SQL.\n\nThis can work, and is an improvement to the data stack, but it also\nloses some of the main appeal of the Semantic Web vision of highly\ndocumented metadata about data.\n\n### The Right Way (TM) is Right In Front Of Us\n\nInstead of all of this complexity, we should just have a standard\nschema language, which can be written in JSON or YAML. It should allow\nus to describe all of the data model in human language. It should make\nit easy for this to be internationalised.\n\nWe could even specify what is derived here *explicitly* separating\nconstraint from inference. And we should keep derived data segregated\nfrom other data so we know the provinence as long as we ensure that\nproperties can not be either derived or concrete with the user being\nunable to distinguish.\n\nAlready most of those using JSON in anger in industry are focusing on\n[JSON Schema](https://json-schema.org/). This solves the easy problems\nfor trees, but not the problems for graphs. It also doesn't solve the\nproblem of describing the meaning of data, or give any help with\ninference or calculation.\n\nWhat we need is a\n[Literate](https://en.wikipedia.org/wiki/Literate_programming) Type\nTheory for JSON, which allows us to specify typed references, giving\nus the power of graphs, but the simplicity of JSON at the same time.\n\nWe should keep the ideas of URIs because without them we will have a\nvery hard time making interoperable data.\n\nBut JSON-LD is more heavy weight than necessary. If we have\ninformation about our data, then we can know if something is a string\nor a reference or a collection and what type of collection. We\nneeden't use something as elaborate as JSON-LD other than perhaps as\nan [elaboration](https://leodemoura.github.io/files/elaboration.pdf).\n\n> The good thing about standards is that there are so many to choose from.\n\u2014 Andrew S. Tanenbaum\n\nI've been working to try out some of these ideas at [TerminusDB](https://github.com/terminusdb/terminusdb) (stars welcome), but for this to work it needs\nto be a standard which is widely shared. And for that reason we need a\nstandard. We need a movement to revitalise the Semantic Web with\n*ideas* that primarily serve to *do work*.\n\nLuckily, there is renewed interest in Semantic Web-like ideas in\nindustry, but industry is not looking to the Semantic Web or to\nacademia to solve them. It's looking to find them in the ideas of the\n[Data Mesh](https://www.datamesh-architecture.com/).\n\n## Academics and Industry\n\nThe political economy of academia and its interaction with industry is\nthe origin of our current lack of a functional Semantic\nWeb.\n\nAcademia is structured in a way that there is very little\nincentive for anyone to build usable software. Instead you are\nelevated for rapidly throwing together an idea, a tiny proof of\nconcept, and to iterate on microscopic variations of this thing to\nproduce as many papers as possible.\n\nIn engineering the devil is in the detail. You really need to get into\nthe weeds before you can know what the right thing to do is. This is\nsimultaneously a devastating situation for industry and\nacademia. Nobody is going to wait around for a team of engineers to\nfinish building a system to write about it in Academia. You'll be\npassed immediately by legions of paper pushers. And in industry, you\ncan't just be mucking about with a system that you might have to throw\naway.\n\nWe have structured collaboration as the worst of both\nworlds. Academics drop in random ideas, and industry try them, find\nthem useless, and move on.\n\nI'm not sure precisely of the right solution to this problem, but I\nthink it genuinely has to change. Innovation in industry is rare and\nhard since deep tech is so high risk. And innovation in academia is as\nlikely to be pie in the sky as the next big thing, because nobody has\nthe capacity to work through the differences between the two.\n\n## The Future of the Semantic Web\n\nThe Future of the Semantic Web is there, the Semantic Web will rise,\nbut it will not be the Semantic Web of the past. Humanity's access to\ndata is of ever increasing importance, and the ability to make\nresilient and distributed methods of curating, updating and utilising\nthis information is key. The ideas which drove the creation of the\nSemantic Web are nowhere near obsolete, even if the toolchain and\ntechnologies which have defined it up to day are fated to go the way\nof the dinosaur.\n", "date": "2023-01-13T19:46:13.034160", "feature": {"@type": "Image", "location": "http://localhost:3000/assets/semantic_web_over_time.png", "alt": "Semantic web interest decreasing over time"}}, {"@type": "Post", "title": "StarWars in TerminusDB", "content": "\nSince TerminusDB recently got GraphQL, I thought I'd do the done thing\nand load in the Star Wars Datset.\n\nFirst, I looked around to find some JSON files that I could marshall\ninto TerminusDB without too much trouble, but as I was looking I came\nacross this DataSet: [Star Wars\nDataset](https://github.com/fgeorges/star-wars-dataset). This not only\nhas JSON files, but RDF!\n\nI thought I'd take a crack at inserting RDF into TerminusDB and then\nquerying it with GraphQL to see what was required.\n\n## RDF To TerminusDB\n\nTerminusDB can read RDF files with little difficulty. However, to get\nthe value of TerminusDB you really need a schema. So first stop is\nreally trying to figure out what we might need to massage about the\nRDF to get it into an easily schemable form.\n\nThe [original starwars ttl](../assets/star-wars-dataset.ttl) is\nactually directly useable, but I don't particularly like when the\nschema, and the data share the same namespace. For instance, here we\nhave `sw` used for both people, and the type of people.\n\n```ttl\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix sw:   <http://h2o.consulting/ns/star-wars#> .\n@prefix xs:   <http://www.w3.org/2001/XMLSchema#> .\n\nsw:people-1  a  sw:People .\nsw:people-1  rdfs:label  \"Luke Skywalker\" .\nsw:people-1  sw:height  \"172\" .\nsw:people-1  sw:mass  \"77\" .\n...\n```\n\nTo get rid of this, I decided to do a bit of sed scripting:\n\n```shell\n$ sed -ie 's/sw:\\([A-Z]\\)/schema:\\1/' star-wars-dataset.ttl\n$ sed -ie 's/ sw:\\([^ ]*\\) / schema:\\1 /' star-wars-dataset.ttl\n$ sed -ie 's/rdfs:label/schema:label'\n```\n\nThis replaces all properties and all types using `sw:` to use\n`schema:`, and remove `rdfs:label`, and replace with a local name\nwhich is just going to look much less ugly when we use GraphQL\n(perhaps later we will allow local renaming of classes and properties\nfor use in GraphQL).\n\nThen we just swap out the prefixes in the header to yield this [file](../assets/star-wars-terminusdb.ttl):\n\n```ttl\n@prefix rdfs: <terminusdb:///schema/star-wars#> .\n@prefix sw:   <terminusdb:///star-wars/> .\n@prefix schema:   <terminusdb:///schema/star-wars#> .\n@prefix xs:   <http://www.w3.org/2001/XMLSchema#> .\n\nsw:people-1  a  schema:People .\nsw:people-1  rdfs:label  \"Luke Skywalker\" .\nsw:people-1  schema:height  \"172\" .\nsw:people-1  schema:mass  \"77\" .\n...\n```\n\nThat's all there is to it!\n\nNow we can load it into terminusdb from the command line. First we\ncreate the database, with schema checking turned off.\n\n```sh\n$ terminusdb db create admin/star-wars --schema=false\n```\n\nNow we can load the turtle:\n\n```sh\n$ terminsudb triples load terminusdb triples load admin/starwars/local/branch/main/instance star-wars-terminusdb.ttl\n```\n\nPresto! We've got a Star Wars Database!\n\n## Making the Schema\n\nNow it is possible to query the database with TerminusDB using\nWOQL. You can do this in the dashboard query panel, perhaps by using the query:\n\n```javascript\nlet [a, b, c] = vars(\"a\", \"b\", \"c\")\nlimit(10,triple(a,b,c))\n```\n\nHowever, TerminusDB is much more useable when you give it a bit of a\nschema first.\n\nWe can open up the [file](../assets/star-wars-terminusdb.ttl) again\nwhich will guide us in creating a schema. We'll open a file called\n`star-wars.json` and start typing:\n\n```json\n[\n    { \"@type\" : \"@context\",\n      \"@base\" : \"terminusdb:///star-wars/\",\n      \"@schema\" : \"terminusdb:///schema/star-wars#\"\n    }\n]\n```\n\nThis first bit is the context. It will tell us what instance and\nschema namespaces we have.\n\nNext thing is to look at our ttl and see what kinds of things are\ndefined:\n\n```ttl\nsw:people-1  a  schema:People .\nsw:people-1  rdfs:label  \"Luke Skywalker\" .\nsw:people-1  schema:height  \"172\" .\nsw:people-1  schema:mass  \"77\" .\nsw:people-1  schema:hair_color  \"blond\" .\nsw:people-1  schema:skin_color  \"fair\" .\nsw:people-1  schema:eye_color  \"blue\" .\nsw:people-1  schema:birth_year  \"19BBY\" .\nsw:people-1  schema:gender  \"male\" .\nsw:people-1  schema:homeworld  sw:planet-1 .\nsw:people-1  schema:film  sw:film-6 .\nsw:people-1  schema:film  sw:film-3 .\n...\n```\n\nThis has a bunch of properties that we should add to our\n`star-wars.json` schema, just below the context, to define\n`schema:People`. Since we have stated in our context that the base for\n`@schema` matches the `schema:` prefix, we don't need to specify it\nanymore in our objects.\n\nThis will look as follows:\n\n```json\n    ...\n    { \"@type\" : \"Class\",\n      \"@id\" : \"People\",\n      \"name\" : \"xsd:string\",\n      \"height\" : \"xsd:decimal\",\n      \"mass\" : \"xsd:decimal\",\n      \"hair_color\" : \"xsd:string\",\n      \"skin_color\" : \"xsd:string\",\n      \"eye_color\" : \"xsd:string\",\n      \"birth_year\" : \"xsd:string\",\n      \"gender\" : \"xsd:string\",\n      \"homeworld\" : \"Planet\",\n      \"film\" : { \"@type\" : \"Set\", \"@class\" : \"Film\" },\n      \"species\" : {\"@type\" : \"Optional\", \"@class\" : \"Species\"},\n      \"starship\" : {\"@type\" : \"Set\", \"@class\" : \"Starship\"},\n      \"vehicle\" : {\"@type\" : \"Set\", \"@class\" : \"Vehicle\"},\n      \"url\" : {\"@type\" : \"Optional\", \"@class\" : \"xsd:anyURI\"},\n      \"label\" : { \"@type\" : \"Optional\", \"@class\" : \"xsd:string\"}\n    },\n    ...\n```\n\nHere we have a number of required properties, together with a number\nof `Set` properties. Sets can have any number of elements of the\nrange. In addition we have a couple of `Optional` properties. These\ncan be anything. The datatypes we use are from\n[xsd](https://www.w3.org/TR/xmlschema-2/), which lines up perfectly\nwith the RDF we have imported.\n\nI repeated this process for `Film`, `Planet`, etc. until I got what I\nfelt was a [complete TerminusDB Star Wars schema](../assets/star-wars.json).\n\nWhen I did the development, I'd add the classes, and properties I\nthought I needed, and then I'd load the schema and have a browse in\nthe UI.\n\nSo:\n\n```shell\n$ terminusdb doc insert admin/starwars -g schema -f < star-wars.json\nDocuments inserted:\n 1: People\n 2: Film\n 3: Planet\n 4: Species\n 5: Starship\n 6: Vehicle\n```\n\nThen will see something like:\n\n![TerminusDB Log Example](../assets/TerminusDB_StarWars.png)\n\nThen when I thought I was completely done, I'd do this:\n\n```shell\n$ terminusdb db update admin/starwars --schema=true\n```\n\nIf there are any problems with the schema, this results in an error which will look something like:\n\n```json\n{\n  \"@type\":\"api:TriplesErrorResponse\",\n  \"api:error\": {\n    \"@type\":\"api:SchemaValidationError\",\n    \"api:witness\": {\n      \"@type\":\"unknown_property_for_type\",\n      \"property\":\"terminusdb:///schema/star-wars#pilot\",\n      \"type\":\"terminusdb:///schema/star-wars#Vehicle\"\n    }\n  },\n  \"api:message\":\"Schema did not validate after this update\",\n  \"api:status\":\"api:failure\"\n}\n```\n\nThis is telling us that we are still missing a `pilot` property for\n`Vehicle`. I missed about 10 or 15 properties in total and then I was done!\n\nI had to modify some brokenness in the `ttl` file too, since some\nproperties were mispelled, and some had ranges which were\ninconvenient, mixing strings and integers for instance. But after\nreloading the triples I was able to do:\n\n```shell\n$ terminusdb update db admin/starwars --schema=true\nDatabase updated: admin/starwars\n```\n\nNow you can fully utilize the document interface in TerminusDB. You\ncan browse through the People, Planets, Vehicles etc from the UI.\n\n## Search using GraphQL\n\nBut we also now get the GraphQL interface. In the `graphql_endpoint`\nexperimental branch in TerminusDB (you can get this using the\nbootstrap by specifying `graphql` in your config) you can navigate to\nthe following address:\n\n```\nhttp://127.0.0.1/graphiql/admin/starwars\n```\n\nYou will then have to set the headers for your authorization. I use\nthe default locally:\n\n```json\n{\"Authorization\": \"Basic YWRtaW46cm9vdA==\"}\n```\n\nYou will get a GraphQL user interface for the StarWars data set! This\nis fully introspective and explorable, with a schema which is\nautogenerated. You can browse through the classes on the left, and use\n`C-c C-c` to get auto-completion.\n\nIt should look something like this:\n\n![TerminusDB Log Example](../assets/TerminusDB_StarWars_GraphQL.png)\n\nHere you can enter in a query, such as:\n\n```graphql\n{\n  Vehicle{\n    manufacturer\n    model\n    url\n    pilot {\n      label\n    }\n  }\n}\n```\n\nThis will get you all of the Vehicles, together with some associated\nspecs, together with all of the pilots who drove them in the Star Wars\nfilms.\n\nWe can also get some rudimetry search. For instance, we can ask for\nthe vehicles in order, taking only 3 of them, sorting by manufacturer.\n\n```graphql\n{\n  Vehicle(limit:3, orderBy: { manufacturer : ASC }){\n    manufacturer\n    model\n    url\n    pilot {\n      label\n    }\n  }\n}\n```\n\nThis results with:\n\n```json\n{\n  \"data\": {\n    \"Vehicle\": [\n      {\n        \"manufacturer\": null,\n        \"model\": \"Fire suppression speeder\",\n        \"url\": \"http://swapi.co/api/vehicles/62/\",\n        \"pilot\": []\n      },\n      {\n        \"manufacturer\": \"Appazanna Engineering Works\",\n        \"model\": \"Oevvaor jet catamaran\",\n        \"url\": \"http://swapi.co/api/vehicles/69/\",\n        \"pilot\": []\n      },\n      {\n        \"manufacturer\": \"Appazanna Engineering Works\",\n        \"model\": \"Raddaugh Gnasp fluttercraft\",\n        \"url\": \"http://swapi.co/api/vehicles/70/\",\n        \"pilot\": []\n      }\n    ]\n  }\n}\n```\n\nNote: `null` is less than any value.\n\nWe can also page these results by setting an offset:\n\n```graphql\n{\n  Vehicle(limit:3, offset: 3, orderBy: { manufacturer : ASC }){\n    manufacturer\n    model\n    url\n    pilot {\n      label\n    }\n  }\n}\n```\n\nAnd we get the next three...\n\n```json\n{\n  \"data\": {\n    \"Vehicle\": [\n      {\n        \"manufacturer\": \"Aratech Repulsor Company\",\n        \"model\": \"74-Z speeder bike\",\n        \"url\": \"http://swapi.co/api/vehicles/30/\",\n        \"pilot\": [\n          {\n            \"label\": \"Luke Skywalker\"\n          },\n          {\n            \"label\": \"Leia Organa\"\n          }\n        ]\n      },\n      {\n        \"manufacturer\": \"Baktoid Armor Workshop\",\n        \"model\": \"Multi-Troop Transport\",\n        \"url\": \"http://swapi.co/api/vehicles/34/\",\n        \"pilot\": []\n      },\n      {\n        \"manufacturer\": \"Baktoid Armor Workshop\",\n        \"model\": \"Armoured Assault Tank\",\n        \"url\": \"http://swapi.co/api/vehicles/35/\",\n        \"pilot\": []\n      }\n    ]\n  }\n}\n```\n\nYou can likewise filter on the individual data fields. For instance,\nwe can write:\n\n```graphql\n{\n  Vehicle(manufacturer: \"Aratech Repulsor Company\"){\n    manufacturer\n    model\n    url\n    pilot {\n      label\n    }\n  }\n}\n```\n\nAnd we will just get the \"74-Z speeder bike\"\n\n```json\n{\n  \"data\": {\n    \"Vehicle\": [\n      {\n        \"manufacturer\": \"Aratech Repulsor Company\",\n        \"model\": \"74-Z speeder bike\",\n        \"url\": \"http://swapi.co/api/vehicles/30/\",\n        \"pilot\": [\n          {\n            \"label\": \"Luke Skywalker\"\n          },\n          {\n            \"label\": \"Leia Organa\"\n          }\n        ]\n      },\n    ]\n  }\n}\n```\n\nOne can poke around and see what queries are possible just by browsing\nthe interface.\n\n## Next Steps\n\nWe're not done with the GraphQL schema generation yet. We're keen to\nget back-links, which make it easy to follow arrows in reverse, path\nqueries, and then complex filtering and full text search. However it's\nalready possible to do quite a lot!\n\nWe're keen to see other example RDF datasets loaded and querable in\nGraphQL with TerminusDB, so if you've anything in mind, drop us a line\nat our [Discord](https://discord.com/invite/Gvdqw97)!\n", "date": "2023-01-13T19:46:13.034160", "feature": {"@type": "Image", "location": "http://localhost:3000/assets/TerminusDB_StarWars.png", "alt": "TerminusDB Log Example"}}, {"@type": "Post", "title": "Syntactic Versioning: What if Git worked with Programming Languages?", "content": "# Syntactic Versioning: What if Git worked with Programming Languages?\n\nI use git most days of the week. And for the vast majority of my\ncommits, this means committing *code*.\n\nSo why would I say something like *What if Git worked with Programming\nLanguages*?\n\nWell, the title is deliberately provocative, but I genuinely mean it,\nand it's not wrong: Git does *not* work with programming\nlanguages. Instead it works with source files of text.\n\nThe fact that git works on *lines of text* has worked fantastically\nwell - better than it has any right too. But is this as far as we can\ngo?\n\n## A Potted History of Version History\n\nI absolutely love git, and I love what it has done to the\ndevelopment process even more.\n\nI remember the bad old days of revision control. I used *file* based\nrevision control on VMS. I've used CVS, subversion (svn), and visual\nsource safe in development environments. The pain and torment these\nsystems caused me has been quite successfully sublimated into my\nunconscious mind, only rising to the surface when I'm forced to think\nabout the history of revision control.\n\nGit is not without its pains - you'll find yourself knee deep in some\nhorrendous merge or rebase at some point, confused how to get things\nworking again. But there are so many benefits over what it was like in\nthe past that these things are easily excused.\n\nThe fundamentally multi-master approach of git, combined with the\nconcepts of CI/CD have yielded enormous benefits. They've actually\nmade my life better.\n\nBut they've also made collaboration better. I'm still learning about\nthe potential to improve software develpment on a weekly\nbasis. In my opinion the well of possibilities in most development\nhouses is probably not yet fully tapped.\n\nAutomatic testing, including unit tests or even very sophisticated\nintegration testing, becomes enormously simpler with the ability to\ncreate the *hypothetical commits* of a pull request. It's essentially\nlike having a [modal\nlogic](https://en.wikipedia.org/wiki/Possible_world) at our disposal\nfor reasoning about correctness. And perhaps someday, these concepts\nwill be made [formal](https://dl.acm.org/doi/pdf/10.1145/2661136.2661137).\n\nYet it would be hubris to think that this is the the end of history\nfor versioning.\n\n## Git for programming languages\n\nThe text-orientated design of git reflects the old unix philosophy of\ncreating simple tools that work generically. And for unix, generic\nmeant: files and text (think sed, awk, pipes etc).  Git builds on\ntechnology that literally goes back to the earliest days of unix, and\nthe `diff` program.\n\nThe *current* version of git is also able to find differences in\nbinary files. But this eschews lines of text for continuous\nstrings. This does not represent a *greater* awareness, but rather a\n*lesser* awareness of the *syntax and semantics*.\n\nProgramming languages are not really just lines of text. Diffs from\ntwo different changes to a source file are not necessarily composable,\neven if there are no overlapping changes. Program code has *syntax*,\nand this syntax is knowable, even in the case of the most dynamic of\nlanguages (such as those with configurable readers such as Common\nLisp). A source file, if it isn't broken, represents a particular\nstructured object, an abstract syntax tree.\n\nWe should be able to leverage this. When we do a diff between two\ndifferent source programmes, we *could* be looking at the alterations\nto the [abstract syntax\ntree](https://www.andres-loeh.de/GDiff.html). And these diffs can even\nbe found, communicated and applied\n[efficiently](https://dl.acm.org/doi/10.1145/3341717).\n\nAnd if we were storing information as ASTs, rather than lines of text,\nwe could store them in a [graph\ndatabase](http://terminusdb.com). Imagine the kinds of testing and\nlinting which could now take place. You could have a full graph query\nlanguage at your disposal to search for problems and perform analysis!\n\nOf course the *unit of analysis* for programming languages has\nremained the text file. [Structure\neditors](https://en.wikipedia.org/wiki/Structure_editor) haven't\nreally taken off yet despite several\n[historical](https://larrymasinter.net/interlisp-ieee.pdf) and\n[contemporary](https://github.com/JetBrains/MPS) attempts.\n\nThere are probably some good reasons for this. Humans expect things\nthat they saw recently adjacent to remain spatially adjacent. Files\ndefinitely act like this, whereas the SmallTalk approach of putting it\nin a database does not. However we don't have to abandon this\nstructure to  get the additional benefits of structure editors. Lists also\nretain order and adjacency.\n\nThere is also the problem of what to do when in a syntactically\ninvalid state. But there are ways around this (think commit) and some\nprogramming environments, (such as\n[agda-mode](https://agda.readthedocs.io/en/v2.6.0.1/tools/emacs-mode.html))\nare good at it.\n\nEditors could benefit from a whole universe of new possibilities if\nthey were storing structured documents at commits, rather than merely\nsource files. If you're worried that you need to put some code\nsomewhere and you don't know what it is yet, this problem can be\nsolved with\n[holes](https://jfdm.github.io/post/2020-07-09-Programming-with-holes.html).\n\nAs code bases grow, the need for more sophisticated linting, for\nbetter approaches to search and retrieval, for looking at all callers\nof a function or procedure, etc. becomes ever more important. What\ncould once be done with grep and a few text files ceases to be\npragmatic when your source tree is several hundred thousands or even\nmillions of lines, with elaborate source trees.\n\nThere are real advantages to going beyond the \"lines of text\" view of\nprogramming languages *in general*, but definitely in terms of version\ncontrol *in particular*.\n\n## The New (and Old) Data Science Tools\n\nWhile there may be advantages as yet unexplored for languages like\npython, there is a new class of very widely used *notebooks* such as\n[Jupyter Notebooks](https://jupyter.org/). These tools are\nintermediate between analytic environments and programming\nenvironments. They store graphics, data and program code together in a\nway that the outcomes can be easily reconstructed. This is a big\nadvantage in data-science where you want to share your results in a\nvisually expressive way. It is also one of the reasons that JetBrains\ndeveloped its MPS structure-editor. It was to facilitate the creation\nof domain-specific languages which could likewise have rich\nenvironments.\n\nAnd while this new approach to data science feels cutting-edge, if we\nlook carefully (and squint our eyes just the right way) we'll\nrecognise that the most widely used [programming language on the\nplanet, Excel](https://github.com/GavinMendelGleason/excel_as_code), is\none of these data-visualisation-programming systems.\n\nUnfortunately, because these new notebooks are *richer* than mere text\nfiles, they also fight with our collaboration tools. Git is\nsignificantly more painful when working with Jupyter notebooks than\nwith raw python. The more we begin to rely on these tools for rapid\ndevelopment of data science analytic results, the more we will need\nadvancement of our version control tools to help us on the\ncollaboration and CI/CD front.\n\n## The Future is Structured\n\nGit's power and advantage over older collaboration and revision\ncontrol tools is clear, but is this the end of the story? I wager it\nisn't.\n\nThe future will be in structured storage, structured diff, structured\nlinting and search and perhaps someday we will even finally get our\nstructure editors.\n\n# Post Script\n\nFor those of you interested in the [discussion on Hacker\nNews](https://news.ycombinator.com/item?id=28670372), I've written a\n[post-script](./IS_IT_TEXT.md) to respond to some of the points and\nobjections.\n", "date": "2023-01-13T19:46:13.034160", "feature": null}, {"@type": "Post", "title": "It turns out, Table Diff is NP-hard, but we tried it anyway", "content": "\nIn programming, there are few things are more distressing than getting\nstuck into a problem only to find it is NP-hard. So it was that I\nstumbled head first into trying to solve the table-diff problem in\ncomplete ignorance of what was in store.\n\nThe first diff algorithm I wrote seemed easy enough. I cribbed the\nalgorithm from something similar I had seen in a paper[^diff]. The\nalgorithm was obviously very non-deterministic / branching but I\ndidn't think too much of it. It wasn't until I tried it out on some\nexamples, using tables of 4 by 4 and 5 by 5 that I realised that the\nnaive approach might not be ok. My solution was taking almost a minute\ncomparing a 5 by 5 table. Going to 10 by 10 appeared to take the\nheat-death of the universe. This kind of scaling behaviour meant that\na diff on a typical Excel file would be uselessly slow.\n\n## First, What is a Diff?\n\nA *diff* is a comparison utility which yields some sort of report,\npatch or command structure about the structural differences between\ntwo objects. The most common diff is of the textual line-based diffs\nfamiliar from revision control systems such as git. These generally\nwork by finding the largest (in some sense) common sequence (of lines\nperhaps) which is shared between two files, and then repeating the\nprocedure to find a minimal *patch* would could bring us from one\nexample to the other.\n\nThe most common line based diff is also a *contextual* diff. Meaning\nthat the patch which is constructed describes the surrounding context\nin which the patch takes place, but does not explicitly describe the\nentire file. This makes the patches more likely to *compose*. That is,\nif I make a patch, and someone else makes a patch, they are likely to\napply in either order as long as we don't overlap in our changes if we\nalso don't also specify precisely what the rest of the file looks\nlike, or precisely what position in the file we want the change. The\nfirst patch might shift everything down by a few lines for instance.\n\nFor example, given the two files:\n\n```shell\n$ cat <<EOF>a.txt\n1\n2\n3\n4\n5\n6\nsome context\nmore context\nfirst\nsecond\nthird\nEOF\n```\n\n```shell\n$ cat <<EOF>b.txt\n1\n2\n3\n4\n5\n6\nsome context\nmore context\nfirst\nthird\nfourth\nEOF\n```\n\n`$ diff -u a.txt b.txt` yields:\n\n```diff\n--- a.txt\t2022-02-14 23:56:58.307338707 +0100\n+++ b.txt\t2022-02-14 23:57:07.751245510 +0100\n@@ -7,5 +7,5 @@\n some context\n more context\n first\n-second\n third\n+fourth\n```\n\nBut a diff needn't be restricted merely to comparing strings. Many\ndata structures can be compared including, trees, lists, and indeed,\ntables.\n\nTree diffs can be more or less complicated depending on if you want\nshared structure and *moves* as well as just inserts and deletions,\nbut in the end they are pretty straight forward and exhibit reasonable\ntime complexity. Diffs of lists can be thought of as closely related\nto the problem of diffs of lines of code. Of course coming up with\n*deep* patches to elements of the lists, where each element is a tree\nmakes things a little more complicated but doesn't present a\nsignificant barrier. And if there are lists within lists... well then\nwe're starting to get the (n-dimensional) table diff problem.\n\n# What is a Table Diff?\n\nA table diff generates a patch from two different matrices of\nvalues. Tables of values are the types of things that you might find\nin an Excel spreadsheet or a CSV.\n\n```javascript\nvar x = [[1,2,3],\n         [4,5,6],\n         [7,8,9]]\nvar y = [[1,2,3],\n         [4,5,6],\n         [7,8,0]]\n# Spot the difference?\n```\n\nNotably, this is *not* the same problem as a database table\ndifference. A database has fixed columns with fixed headers of a fixed\ntype - the order doesn't matter but the column names do. This problem\nis equivalent to the tree diff with lists of leaves. We can basically\nsolve this the same way as we solve for strings.\n\n```javascript\n{ \"column1\" : [1,2,3],\n  \"column2\" : [\"a\",\"b\",\"c\"] }\n```\n\n## Why is this hard?\n\nThe problem is that to compare the two matrices, we need to slide\nevery sub-window of every matrix over every other sub-window to find\nwhere they maximally overlap. This is clearly going to be a bit time\nconsuming to say the least. But the *proof* that this is NP-hard comes\nfrom the observation that solving this problem would solve another\nproblem, the [Graph Clique\nProblem](https://en.wikipedia.org/wiki/Clique_problem).\n\nOnce I saw computations times spinning out of control, I started to do\na bit more research looking at the literature to solve the problem in\nprinciple. To my amazement there was *very* little practical\nliterature that I could find written on the subject. One would think\nthat comparing two matrices would be a better studied problem!\nEspecially as it appears to be necessary to find a diff for something\nas prosaic as Excel.\n\nFortunately there is a good paper on the so called Generalized\nLCS[^glcs] (Longest Common Subsequence) problem. The paper notes that\nsince you can reduce a graph to an adjacency matrix, you can solve the\nclique problem by solving the gernalized LCS over matrices. A clever\nway of reducing the problem to another intractable problem. The paper\nalso shows that trees present no serious difficulties (at least\ntheoretically speaking).\n\n## Perfect is the Enemy of Good\n\nOk, so now that we're well and truly screwed, what do we do? Well, the\nproblem we are having is one of finding a *maximal* answer. In\npractice what we want is simply a *good* answer. It doesn't have to be\nthe absolute best answer because what we're trying to do is merely to\nmake the changes comprehensible.\n\nThat means we might be able to find a way out by getting solutions\nthat are sub-optimal but *good enough*. Hopefully we can use enough\ntricks and heuristics to escape our conundrum.\n\nLuckily there is a *lot* of literature on tricks for solving\nNP-complete problems with heuristics. We couldn't find anything that\nsolved the precise problem but we found some hints and tricks that we\nfound helpful.\n\n## Russian Dolls\n\nThe first trick was inspired by *Russian Doll Envelopes*. This is a\ncommonly used attack strategy in combinatorial optimization.  We want\nto find maximal rectangles that exist in both matrix A and Matrix\nB. We can choose an area size A, and then test all windows between A\nand B. If we find a match, then we can try a bigger area. Using a\n[binary search](https://en.wikipedia.org/wiki/Binary_search_algorithm)\nwe can find the largest overlap in a log-like way. Once we've found\nthe largest, we *know* that we can exclude this region, and since all\nsmaller areas are subsumed by this one, we can continue the search on\nsub-problems.\n\n## Moves Matter\n\nThe other trick to notice is that in a table diff, you really don't\nwant the same sort of contextual locking that you do in text. What\npreceeds and what follows is probably not critical. Take for instance\nan excel file that looks like this:\n\n```\nName   | Birthdate | Position\nJoe    | 10-12-91  | Jr. Engineer\nJill   | 05-20-82  | Sr. Engineer\n```\n\nIf we reorder the Name and Birthdate columns, this shouldn't be\nrecognised as a *move* Similarly, if we sort the rows, we will want\nthese rows to be seen as *moves* rather than deletion and insertion of\nnew data.\n\nThis means we can compare the remaining windows without having to\nworry too much about our neighbours.\n\n## Choosing indices\n\nThe last trick is in choosing indices for our window comparisons. We\nwant to be able to keep choosing windows of regions which are not\n*excluded* by maximal overlaps which have already been found. This is\na constraint satisfaction problem over inequality constraints in two\ndimensions.\n\nWe used SWIPL's excellent clp(fd) library to solve this problem in a\nrelatively naive fashion, though we are still exploring a faster\nsolution with a custom constraint algorithm in rust.\n\n## Escaping the Maze\n\nLuckily, with a bit of elbow grease we were able to make the diff\nalgorithm practial for Excel spreadsheets which you might find in the\nwild, rather than 10 by 10 toy problems.\n\nHubris sometimes has its rewards!\n\n[^diff]: [Type-safe diff for families of\n    datatypes](https://www.andres-loeh.de/GDiff.html), Eelco Lempsink,\n    Sean Leather, Andres L\u00f6h\n[^glcs]: [Generalized LCS](https://www.researchgate.net/publication/227255331_Generalized_LCS). Amir, Amihood & Hartman, Tzvika & Kapah, Oren & Shalom, Braha & Tsur, Dekel. (2007).\n", "date": "2023-01-13T19:46:13.034160", "feature": null}, {"@type": "Post", "title": "A Blogging Platform in TerminusDB", "content": "\nHow easy is it to make a blog-focused CMS in TerminusDB?\n\nVery!\n\nThis\n[terminusBlog](https://github.com/GavinMendelGleason/terminusBlog) is\ndesigned to show how easy it is to build a custom web-app using\nTerminusDB from scratch.\n\nYou should be able to clone the repository and run it locally provided\nyou have\n[npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\nand [terminusdb](https://github.com/terminusdb/terminusdb) installed.\n\nJust run:\n\n```shell\nmake\nnpm start & terminusdb serve &\n```\n\nAnd you should be able to view the blog at `localhost:3000`. The\nTerminusDB backend will be available at `localhost:6363`.\n\n## The Schema\n\nWhat do we want to store in a blog?  Typically we have: Authors, Posts, SiteMap,\nand Pages. Let's start with those...\n\n```javascript\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Author\",\n  \"name\" : \"xsd:string\",\n  \"avatar\" : \"xsd:anyURI\"\n  \"bio\" : \"xsd:string\" }\n```\n\nOur author is just a name, a bit of text describing who they are, and\na URI which we will use to display an image.\n\n```javascript\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Post\",\n  \"title\" : \"xsd:string\",\n  \"content\" : \"xsd:string\",\n  \"date\" : \"xsd:dateTime\",\n  \"author\" : { \"@type\" : \"Set\", \"@class\" : \"Author\" } }\n```\n\nPosts have a title, the actual content of the blog post, and a set of\npossible authors (since sometimes you have more than one person\nwriting a piece) and when it was made.\n\n```\n{ \"@type\" : \"Class\",\n  \"@id\" : \"Page\",\n  \"title\" : \"xsd:string\",\n  \"content\" : \"xsd:string\" }\n```\n\nNot much to a page! Just a timeless post.\n\n```javascript\n{ \"@type\" : \"Class\",\n  \"@id\" : \"SiteMap\",\n  \"items\" : { \"@type\" : \"Set\", \"@class\" : \"SiteItem\" }}\n\n{ \"@type\" : \"Class\",\n  \"@id\" : \"SiteItem\",\n  \"name\" : \"xsd:string\",\n  \"location\" : \"xsd:anyURI\" }\n```\n\nAnd a site map is just a set of items, with a name and location.\n\nWe can put all of this in a file, [schema.json](schema.json) and then (assuming we've already installed TerminusDB) load it with the command:\n\n```shell\nterminusdb store init\nterminusdb db create admin/blog\nterminusdb doc insert admin/blog -g schema -f < schema.json\n```\n\nTerminus will respond with:\n\n```shell\nDocuments inserted:\n 1: Author\n 2: Post\n 3: Page\n 4: SiteMap\n 5: SiteItem\n```\n\n## Layout\n\nNow we want to actually design our website. First, let's create a\nbasic structure.\n\nAssuming you have [npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) installed, you can run the following to start a react app.\n\n```shell\nnpx create-react-app blog\n```\n\nI would also like to use GraphQL, so I'll install the Apollo client.\n\n```shell\nnpm install @apollo/client\n```\n\nAnd since I hate writing HTML, and write all of my blogs in Markdown,\nI'm going to use a Markdown renderer.\n\n```shell\nnpm install react-markdown\nnpm install remark-gfm\nnpm install react-syntax-highlighter\nnpm install parse-numeric-range\n```\n\nReact markdown is really great. It makes your source code look very\nreadable and deals with a wide variety of formats. To make it look\nreally beautiful however, I had to crib some tricks from [Amir\nArdalan's blog on\nMarkdown](https://amirardalan.com/blog/syntax-highlight-code-in-markdown).\n\nNow we're basically ready to go, we just need to edit our [App.js](../assets/App.js) file\nand give ourselves a river of news.\n\n```jsx\nfunction PostRiver() {\n  const offsets = get_offsets()\n  const { loading, error, data } = useQuery(POSTS_QUERY, {variables:offsets});\n  if (loading) return <Loading />;\n  if (error) return `Error! ${error.message}`;\n  return (\n    <div>\n      <div name='post_river'>\n      {data.Post.map((post) => {\n          console.log(post)\n          const date_time_obj = new Date(post.date);\n          var options = { weekday: 'long', year: 'numeric', month: 'long', day: 'numeric' }\n          var date_time = date_time_obj.toLocaleDateString(\"en-US\", options)\n          var id = post.id.replace(/^iri:\\/\\/data/, '')\n          var path = `${id}`\n          var content = snippit(post.content) + `... **[see more](${path})**`\n          var image = post.feature ? Image(post.feature) : ''\n          return (\n           <div key={id} id={id} name='BlogCard'>\n             <table className='blogTable'>\n              <tr>\n                <td className='blogData'>\n                  <span><h2><a href={path}>{post.title}</a></h2></span><em>{date_time}</em>\n                  <ReactMarkdown components={MarkdownComponents}>\n                  {content}\n                  </ReactMarkdown>\n                </td>\n                {image}\n              </tr>\n             </table>\n             <hr />\n          </div>\n          )})}\n      </div>\n      <More />\n    </div>\n  );\n}\n```\n\nOk, so that has a lot in it.\n\nThe first bit, `get_offsets()` just graphs the current page if we're\nnot the home page, but have pressed the `more...` button.\n\nWe do a bit of date presentation stuff, some id munging, trimming of\nthe post to get a snippit, and then the actual blog synopsis card.\n\nWe do a GraphQL query to get the posts we are interested in, namely:\n\n```graphql\n query PostsQuery($offset: Int, $limit: Int) {\n    Post(offset: $offset, limit: $limit, orderBy: { date : DESC }) {\n        id\n        date\n        title\n        content\n        feature {\n           alt\n           location\n        }\n    }\n}\n```\n\nThis gives us a feature image to go with our synopsis (if one exists).\n\n## Router\n\nWe need to serve up a few pages, so its convenient to put in a router.\n\n```shell\nnpm install react-router-dom\n```\n\nThe main [App()](../assets/App.js) is pretty straight forward.\n\n```js\nfunction App() {\n  let routes = useRoutes([\n    { path: \"/\", element: <Posts /> },\n    { path: \"p\", children : [\n       { path: \":page\", element: <Posts /> }]},\n    { path: \"Post\", children : [\n       { path: \":id\", element: <SinglePost /> }]},\n    { path: \"Page\", children : [\n       { path: \":id\", element: <SinglePage /> }]}\n  ]);\n  return routes;\n}\n```\n\nWe have the home page, further down the blog stream, a single post\ndisplay, and individual page display.\n\n## The SiteMap\n\nThe SiteMap query gives back an ordered list of items which have a\nname and a location. We use the `order` field, an int, to make sure we\nget them back as an ordered list.\n\n```graphql\nquery SitemapQuery {\n    SiteMap {\n        items(orderBy: { order : ASC }) {\n           id\n           name\n           location\n        }\n    }\n}\n```\n\n## Getting Some Data In\n\nTo get the data in, I just wrote a short python\n[script](../assets/create_from_files.py) that grabbed my already\nexisting blogs.\n\nSince the files are already Markdown, I just stick them in a content\nfield, extract a title and feature image using regexp magic and guess\na datetime from the unix timestamp of the file.\n\n## Reflections\n\nI never really used the Author object, as I'm the only Author, but\notherwise it's a fairly useable blogging engine as is.\n\n![Blog engine in TerminusDB](../assets/blog.png)\n\nI'm keen to put this up on TerminusX, and try to host the front end on\nsomething like [Heroku](https://www.heroku.com/).\n\nThe whole thing demonstrates that TerminusDB makes a flexible GraphQL\nbackend for a content management system, with very little effort from\nthe developer.\n", "date": "2023-01-13T19:46:13.034160", "feature": {"@type": "Image", "location": "http://localhost:3000/assets/blog.png", "alt": "Blog engine in TerminusDB"}}, {"@type": "Post", "title": "TerminusDB CLI for Push / Pull / Clone", "content": "\nTerminusDB is designed to be a distributed database with a\ncollaboration model designed to rhyme with the ideas behind git. It is\nessentially meant to be a *git for data*.\n\nThe building blocks of the model are:\n\n* Revision Control: We have commits for every update\n* Diff: Differences between commits can be interpreted as patches between states.\n* Push/Pull/Clone: We can communicate diffs between nodes using push / pull / clone.\n\n## Clone\n\nThe simplest way to see how the TerminusDB CLI can be used is to clone\na resource from TerminusX. This is analogous to creating a repository on GitHub.\n\nFirst we log into TerminusX, create a new data product, and make sure\nwe have an access token to the team in which we created that data\nproduct. Then copy the URL to clone from the data product info page.\n\nSupposing I make a data product called `example` in the team\n`Terminators`. We could then issue the following command using the\nTerminusDB CLI.\n\n```shell\n./terminusdb clone 'https://cloud-dev.terminusdb.com/Terminators/example' --token='XYZ'\n```\n\nOnce completed, you'll have a local copy of this database!\n\n```shell\n./terminusdb list\nTerminusDB\n\u2502\n\u2514\u2500\u2500 admin/example\n    \u2514\u2500\u2500 main\n```\n\nNow we can put something interesting in our database. First, let's create the following schema.\n\nFor a file `schema.json`:\n```javascript\n{ \"@id\" : \"Person\",\n  \"@type\" : \"Class\",\n  \"name\" : \"xsd:string\",\n  \"occupation\" : \"xsd:string\",\n  \"friends\" : { \"@type\" : \"Set\",\n                \"@class\" : \"Person\" }\n}\n```\n\n```shell\n./terminusdb doc insert admin/example --graph_type=schema --message='adding base schema' < schema.json\n```\n\nNow that we have the schema we can go ahead and submit a\ndocument. Let's start with something simple.\n\n```shell\n./terminusdb doc insert admin/example --message='adding Gavin' --data='{\"@type\" : \"Person\",\"name\" : \"Gavin\", \"occupation\" : \"Coder\"}'\n```\n\nNow let's take a look at our history:\n\n```shell\n./terminusdb log\n\nb10d1z9vzp060utfa4rtptt823woskf\n----------------------------------\nDate: 2022-04-25T11:02:07+00:00\nAuthor: admin\nMessage: adding Gavin\n\nafcy8b5p86m16fpnh3b7ktp3zfpaku\n----------------------------------\nDate: 2022-04-25T11:01:59+00:00\nAuthor: admin\nMessage: adding base schema\n```\n\nGreat! We've our schema update and our document insertion. Now that we have some new data, we can have a go at pushing.\n\n## Push\n\nFirst, let's see what kinds of switches we have with push:\n\n```shell\n./terminusdb push\n\nterminusdb push DB_SPEC\n\nPush a branch.\n\n--help           -h  boolean=false  print help for the `push` command\n--branch         -b  atom=main      set the origin branch for push\n--remote-branch  -e  atom=_         set the branch on the remote for push\n--remote         -r  atom=origin    the name of the remote to use\n--prefixes       -x  boolean=false  send prefixes for database\n--token          -t  atom=_         machine access token\n--user           -u  atom=_         the user on the remote\n--password       -p  atom=_         the password on the remote\n```\n\nHere we see that we can define a remote for the push command. Since we\ncloned the database, `origin` will already be the correct remote. We\ncan see this by typing:\n\n```shell\n./terminusdb remote get-url admin/example\n\nRemote origin associated with url https://cloud.terminusdb.com/Terminators/Terminators/example\n```\n\nBecause of this, our push command just needs our authentication token (the one we used to clone).\n\n```shell\n./terminusdb push admin/example --token='...'\n\nadmin/example pushed: new(\"71030a31c7057e6cd9cb9e354ede032717023aa6\")\n```\n\nGreat! We now have our data on the TerminusX!\n\n## Managing Change\n\nWe can now go in to the server and create a new Person, Jane through\nthe document UI in TerminusX. Once this is done, we can then do the following:\n\n```shell\n./terminusdb pull admin/example --token='...'\nadmin/example pulled: status{'api:fetch_status':true,'api:pull_status':\"api:pull_fast_forwarded\"}\n```\n\nNow we can dump the documents and see what is in there:\n\n```shell\n./terminusdb doc get admin/example\n{\"@id\":\"Person/23d01a9462711b84029147fd0a92611174023de946e00bdc2fb79b44e25e48f5\", \"@type\":\"Person\", \"name\":\"Gavin\", \"occupation\":\"Coder\"}\n{\"@id\":\"Person/c2a53dec09e9805593b978ecca7f73cecb18cddae70645e7037d202d2a9fd185\", \"@type\":\"Person\", \"friends\": [\"Person/23d01a9462711b84029147fd0a92611174023de946e00bdc2fb79b44e25e48f5\" ], \"name\":\"Jane\", \"occupation\":\"Nuclear Physicist\"}\n```\n\nLook at that! We've synced our changes.\n\nAnd if we look at the log...\n\n```shell\n./terminusdb log admin/example\n\nijrcvs8el838we97vl3p19vu4g6me7q\n--------------------------------\nDate: 2022-04-25T11:52:18+00:00\nAuthor: gavin@terminusdb.com\nMessage: Adding a new instance of type Person\n\nbb92atqjkqx40linuwxlp5y0jlpawkc\n--------------------------------\nDate: 2022-04-25T11:48:42+00:00\nAuthor: admin\nMessage: adding Gavin\n\ntwg58rv4ohaw887k7ewej2j1hhm1pwt\n--------------------------------\nDate: 2022-04-25T11:48:36+00:00\nAuthor: admin\nMessage: adding base schema\n```\n\n## Conclusion\n\nThe CLI allows us to directly modify a store, whether the server is\nrunning or not. This is possible due to the immutable datastorage\napproach token in TerminusDB and is a pretty cool feature of\nimmutability.\n\nWe can also sync these stores and keep history of edits made remotely.\n\nIn a following article I'll show how we can use `pull` with diverging\nhistories, for when some edits have taken place which result in\nconflicts.\n\nHappy terminating!\n", "date": "2023-01-13T19:46:13.034160", "feature": null}, {"@type": "Post", "title": "TerminusDB v10.1.0: The Mule", "content": "\nWe have recently release TerminusDB v10.1 which we have labelled The\nMule. We have added a number of technical features, and performance\nenhancements, but most of these are all pieces on the way to realising\nour broader vision.\n\nOur aim is to create a distributed database for knowledge graphs. One\nin which you can incrementally grow segments of the graph (data\nproducts) over many nodes creating individual high quality products\nthat are linked at the boundaries (in a manner not entirely unlike how\nobject code linking works) and can be shared between the nodes. We\nwant a truly distributed, multi-party, scalable knowledge graph\nmanagement system.\n\nTo facilitate this, we have made a number of somewhat unusual\ntechnical choices which diverge from standard database technology.\n\n* The database is immutable, with alterations to the database stored\n  as deltas.\n\n* We use succinct data structures to make sure our updates have\n  a compact representation, facilitating sharing between nodes, and\n  loading of extremely large graphs into memory, which avoids\n  thrashing (something which graphs are particularly good at).\n\n* We keep metadata in a *commit graph* about histories for each data\n  product at a node. and the data which has changed.\n\n* We share our changes and histories by sharing these commit graphs\n  along with the changes they refer to.\n\n* We structure our units as *objects* (with a natural JSON\n  representation) with links (somewhat analogous to web-pages), butwe\n  store and can query everything as a graph.\n\n* Distributed transactions are \"slow\", and we manage them in a fashion\n  analogous to git, with merges and conflict resolution as the approach.\n\n* Search is provided using a datalog query engine which makes graph\n  search convenient.\n\nNot everything that is necessary for real industrial scale production\nof the distributed knowledge graph is there yet. We still have\nimportant steps on our roadmap before this achieved.\n\nHowever, we're starting to become very strong in the creation of\nindividual domain focused knowledge graphs. The technical improvements\nwhich have made this convenient include: document diff, type\ninference, capture ids, document UI, and unconstrained JSON fields.\n\n## Diff\n\nIn order to have the \"slow\" distributed transactions mentioned before,\nwhich allow us to modfiy graphs using rebase, cherry pick, merge etc\nfor strutured documents, we really need to have a diff\nalgorithm. Previously, diffs in TerminusDB were purely a result of\ndifferences in the set of triples. This was awkward from the point of\nview of object identity, which is more commonly how people think about\ntheir data.\n\nThe diff interface with TerminusDB now provides uses JSON documents as\nthe unit of analysis. It performs a tree structured diff on\ndictionaries, and a list diff on lists. All datatypes are currently\nconsidered atomic, but we would like to introduce diffs at the\ndatatype level in the future (for instance for strings).\n\n```javascript\nval x = {\n  '@id': 'Example/a',\n   a: 'pickles and eggs'\n}\n\nval y = {\n  '@id': 'Example/a',\n   a: 'vegan sausage'\n}\n\n# diff between x and y\n{\n  '@id': 'Example/a',\n   a: {\n          '@after': 'vegan sausage',\n          '@before': 'pickles and eggs',\n          '@op': 'SwapValue',\n      }\n}\n```\n\n## Capture Ids\n\nIn TerminusDB, transactions always generate data as a single function\nof the current state of the world. There are no intermediate states\navailable in a query.\n\nThis presents a bit of a problem if I want to add a link to a document\nwhich isn't there yet. Or perhaps we want to add two documents which\nrefer to each-other.\n\n```json\n{ \"@type\" : \"Person\",\n  \"name\" : \"Joe\",\n  \"friends\" : ?Jim }\n\n{ \"@type\" : \"Person\",\n  \"name\" : \"Jim\",\n  \"friends\" : ?Joe }\n```\n\nIt is possible to use a well chosen document id naming scheme to avoid\nthis problem, but it is still awkward. TerminusDB uses a number\npre-built ID generation schemes (lexical keys, hash keys and\nrandom). And sometimes it is difficult to even calculate what the\ncorrect ID is, and it is nicer to leave it to TerminusDB to figure it out.\n\nCapture ids make it easy to do provide this sort of forward reference.\n\n```json\n{ \"@type\" : \"Person\",\n  \"@capture\" : \"Joes_ID\",\n  \"name\" : \"Joe\",\n  \"friends\" : { \"@ref\" : \"Jims_ID\" } }\n\n{ \"@type\" : \"Person\",\n  \"@capture\" : \"Jims_ID\",\n  \"name\" : \"Jim\",\n  \"friends\" : { \"@ref\" : \"Joes_ID\" } }\n```\n\nThe naming schema for the capture can be chosen in any way that is\nconvenient, making it straightforward to load complex interconnected\ngraphs from JSON quickly.\n\n## Type Inference\n\nSpecifying the types of every document can be inconvenient. And for\nsubdocuments, in which the type is unambiguous it is particularly\nirritating.\n\nSo we've added a quite general system of type inference which allows\nthe insertion of documents when there is *precisely one* type for a\ndocument. We might be able to insert a person documents as:\n\n```json\n{ \"name\" : \"Joe\",\n  \"friends\" : \"Person/Jim\" }\n```\n\nprovided no other type can be formed from a `\"name\"` field of type\nstring, and a `\"friends\"` field which points to a person.\n\n## Unconstrained JSON\n\nTerminusDB started with the goal of schema first. The reason for this\ndecision was experience in dealing with complex but unconstrained\ndata. Garbage in - Garbage out, so if you don't know you are putting\ngarbage in, you are in trouble.\n\nHowever, in practice there are numerous reasons you might want to\nstore unconstrained data too. Not least because you got the data from\nsomeone who did not constrain it, and perhaps you might even want to\nlater clean it but only incrementally.\n\nAnd sometimes, the specification of some JSON interchange standard is\nso weak in parts, that it can't really be feasibly modelled.\n\nIn this case we need a way to add unconstrained JSON. In the Mule\nrelease, TerminusDB supports unconstrained JSON as a datafield of a\nproperty, or directly as an object.\n\n## Document UI\n\nWe have built a toolkit which makes it much more convenient to provide\ndocument curation interfaces. It helps to automatically stucture the\ndisplay, editing and submission of documents, including with\ngeolocation information.\n\nBuilding knowledge graphs is in practice often a mixture of writing\ningestion which connects data from various sources, automated\nenrichment and cleaning procedures, and hand curation. We are trying\nto make TerminusDB convenient for all of these workflows.\n\n## The Future: What's next\n\nThe next minor release of TerminusDB will have big performance\nimprovements, especially on document retrieval times.\n\nAfter that we will begin to work on the scaling features in\nanger. Specifically making it possible to load and query larger\nfederated collections of information conveniently.\n\nAnd of course, we want to prioritise what our community thinks is\nimportant. So if you have ideas for TerminusDB, we're very open to\nsuggestions.\n", "date": "2023-01-13T19:46:13.034160", "feature": null}, {"@type": "Post", "title": "Why TerminusX", "content": "# [Why TerminusX](https://terminusdb.com/)\n\nI feel some relief and a lot of trepidation that I'm finally seeing\nthe launch of the TerminusX cloud service. Some very long nights and\ntoo many days have gone into making this possible.\n\nThe TerminusX cloud service is built on the substantially over-hauled\nand backward incompatible TerminusDB v10. Since we have changed things\nso fundamentally, its only fair to explain *why*. Of course the pithy\nexplanation is: *developer experience*.\n\nBut the long explanation is a little more complicated.\n\n## A versioned Knowledge Graph for Collaboration\n\nTerminusDB was designed to be a knowledge graph with the concept of\nversion control built into the core. This idea developed out of\nrequirements that we encountered when creating the infrastructure for\nthe [Seshat Global History Databank](http://seshatdatabank.info/),\nwhich needed to store complex structured data with interconnections\n*and* be possible for hundreds of practictioners to clean, edit and\nenhance. This seemed like a good job for both a graph and versioning.\n\nAnd so we set out building a versioned graph.\n\n## Too complex\n\nWe built several large and complex knowledge graphs using our\ntool. Seshat, a large knowledge graph of corporate intelligence in\nPoland and a supply chain management tool with predictive analytics\ndrawing from the graph. However in every case, the core database team\nneeded to be intimately involved in the design and deployment. We\ncouldn't find consultants, or in-house teams that were confident to\nforge ahead on an unusual database which had no entries on\nStackOverflow.\n\n## Open Source\n\nWe decided we needed to be open source if we wanted to get the kind of\nbroader knowledge required to use our tool effectively. So we released\nit to the public, began to grow the community and created a python and\njavascript client to encourage application development on the back of\nTerminusDB.\n\nWe also spent a lot of time writing very elaborate documentation to\ndescribe how to use the product. This was a surprisingly difficult\ntask as complex tools require a lot of documentation and the people\nwho are capable of documenting it are often the people who would\notherwise be writing the code.\n\nAnd we got a fair few users to join our community, ask questions and\nplug away at applications. But we heard over and over, the same\nproblems occuring. The complexity would eventually overwhelm newbies\nand they would churn.\n\n## Simplification\n\nWe had always viewed TerminusDB as a *document graph*. That is, while\nevery edge in the system was a genuine edge in the graph (in the RDF\ntradition rather than the property-graph tradition), we viewed\n*segments* of the graph as belonging to specific objects which could\nbe treated as related.\n\nOne of our early design choices was to use OWL to represent the\n*shape* of the graph. This made some sense because OWL is a very rich\nlanguage for describing graphs. However it also has some drawbacks. It\nis very hard for developers to read OWL - even very smart\ndevelopers. It also was never built to describe *schemata* but rather\n*ontologies* (to describe what *could* be represented, rather than\nwhat *must* be represented). It also had no concept of a document, so\nwe had to graft one onto it, again a source of confusion for our users.\n\n## Collaboration\n\nMany people also liked the *concept* of our distributed, git-like,\nmultimaster setup with TerminusDB (this hasn't gone away...) However\nwhen it came down to it, most people really just don't want to set up\na database anymore. They'd rather use one in the cloud. They don't\nwant to worry about provisioning or scaling. They'd rather someone\nelse deal with these problems. And really who is better at dealing\nwith the scaling problems of a database than the people who made it?\n\nWe realised that while a git-like hub might be useful at some point\n(after people have built lots of shareable data products) what we\nreally needed was a cloud database that people could use.\n\n## TerminusX\n\nEventually we felt we could no longer face our users judgement without\nchanging things. We decided to simplify the interface, make the\nconcept of the *document* more central, make the primary interaction\nmethod be through *json* documents and create a schema language that\nlooks like the JSON you hope to build (and feels more like one you\nmight write in a programming language).\n\nIt's very early days, but *internally* to the team, the traction has\nalready been noticable. Many more of our staff have built complex\nknowldge graphs using our software. We have also trialed the software\nwith some private beta users, and the feedback has already been\nencouraging.\n\nThe possibilities that a document graph could bring to application\ndevelopment has always excited me. I really feel like their day will\ncome. Its part of why I've been involved with implementation of graph\ndatabases since 2004.\n\nThe object store of yore always *felt* like a good idea to me, but I\nwas aware of its problems: difficulties with referential integrity,\ndifficulties with concepts of identity etc. These former attempts did\nnot succeed but they gave us great information as to what *might*\nsucceed in the future.\n\nWe have learned those lessons, and now I hope we have learned the\nlessons from our users.\n\nAn inter-connected, browsable system of data should be made easy for\ndevelopers using the kind of tools they are already comfortable with.\n\nI feel like we are finally approaching that future.\n\n", "date": "2023-01-13T19:46:13.034160", "feature": null}]